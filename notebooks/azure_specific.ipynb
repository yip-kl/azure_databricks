{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mount storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageAccount=\"xxx\"\n",
    "mountpoint = \"/mnt/xxx\"\n",
    "storageEndPoint =\"abfss://rawdata@{}.dfs.core.windows.net/\".format(storageAccount)\n",
    "print ('Mount Point ='+mountpoint)\n",
    "\n",
    "#ClientId, TenantId and Secret is for the Application(ADLSGen2App) was have created as part of this recipe\n",
    "clientID =\"xxx\" #Called as Application Id as well\n",
    "tenantID =\"xxx\"\n",
    "clientSecret =\"xxx\"\n",
    "oauth2Endpoint = \"https://login.microsoftonline.com/{}/oauth2/token\".format(tenantID)\n",
    "\n",
    "\n",
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "           \"fs.azure.account.oauth2.client.id\": clientID,\n",
    "           \"fs.azure.account.oauth2.client.secret\": clientSecret,\n",
    "           \"fs.azure.account.oauth2.client.endpoint\": oauth2Endpoint}\n",
    "\n",
    "try:\n",
    "  dbutils.fs.mount(\n",
    "  source = storageEndPoint,\n",
    "  mount_point = mountpoint,\n",
    "  extra_configs = configs)\n",
    "except Exception as e:\n",
    "  if 'Directory already mounted' in str(e):\n",
    "    print('Directory already mounted')\n",
    "  else:\n",
    "    print(str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from EventHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions  import from_unixtime\n",
    "from pyspark.sql.functions  import to_date\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import to_json, struct\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The $ConnectionString and $Default are fixed values, don't update them\n",
    "TOPIC = \"Event Hub namespace\"\n",
    "BOOTSTRAP_SERVERS = \"Host name of Event Hub:9093\"\n",
    "CONN_STRING = \"Go to Event Hub's Shared Access Policies -> Click onto a policy -> Copy Connection stringâ€“primary key here\"\n",
    "EH_SASL = EH_SASL = f\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"$ConnectionString\\\" password=\\\"{CONN_STRING}\\\";\"\n",
    "GROUP_ID = \"$Default\" \n",
    "\n",
    "# // Read stream using Spark SQL (structured streaming)\n",
    "# // consider adding .option(\"startingOffsets\", \"earliest\") to read earliest available offset during testing\n",
    "kafkaDF = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"subscribe\", TOPIC) \\\n",
    "    .option(\"kafka.bootstrap.servers\", BOOTSTRAP_SERVERS) \\\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n",
    "    .option(\"kafka.sasl.jaas.config\", EH_SASL) \\\n",
    "    .option(\"kafka.request.timeout.ms\", \"60000\") \\\n",
    "    .option(\"kafka.session.timeout.ms\", \"60000\") \\\n",
    "    .option(\"kafka.group.id\", \"POC\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\ \n",
    "    .load() \\\n",
    "    .withColumn(\"source\", lit(TOPIC)) # Optional: Also add the topic as column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if streaming is on and getting the schema for the kakfa dataframe \n",
    "print(kafkaDF.isStreaming)\n",
    "print(kafkaDF.printSchema())\n",
    "\n",
    "#It should then output something like this:\n",
    "#\n",
    "#True\n",
    "#root\n",
    "# |-- key: binary (nullable = true)\n",
    "# |-- value: binary (nullable = true)\n",
    "# |-- topic: string (nullable = true)\n",
    "# |-- partition: integer (nullable = true)\n",
    "# |-- offset: long (nullable = true)\n",
    "# |-- timestamp: timestamp (nullable = true)\n",
    "# |-- timestampType: integer (nullable = true)\n",
    "# |-- source: string (nullable = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the Kafka message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the key and value\n",
    "newkafkaDF=kafkaDF.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Creating the schema for the vehicle data json structure\n",
    "jsonschema = StructType() \\\n",
    ".add(\"id\", StringType()) \\\n",
    ".add(\"timestamp\", TimestampType()) \\\n",
    ".add(\"rpm\", IntegerType()) \\\n",
    ".add(\"speed\", IntegerType()) \\\n",
    ".add(\"kms\", IntegerType()) \n",
    "newkafkaDF=newkafkaDF.withColumn('vehiclejson', from_json(col('value'),schema=jsonschema))\n",
    "\n",
    "# Flatten the json\n",
    "kafkajsonDF=newkafkaDF.select(\"key\",\"value\", \"vehiclejson.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the streaming data to Delta tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_destination = \"/mnt/Blob/Vehicle_Chkpoint_raw/\"\n",
    "delta_table = \"VehicleDetails_Delta\"\n",
    "\n",
    "# Save raw data as-is in delta format. Checkpoint is set so that it can recover from failure in the event of server failure\n",
    "\n",
    "kafkajsonDF.selectExpr(\n",
    "                  \"id\"\t  \\\n",
    "                  ,\"timestamp\"\t   \\\n",
    "                  ,\"rpm\"\t\\\n",
    "                  ,\"speed\" \\\n",
    "                  ,\"kms\" \\\n",
    "                  ,\"source\" ) \\\n",
    ".writeStream.format(\"delta\") \\\n",
    ".outputMode(\"append\") \\\n",
    ".option(\"checkpointLocation\", data_destination) \\\n",
    ".option(\"mergeSchema\", \"true\") \\\n",
    ".start(\"/mnt/Blob/Vehicle_Raw\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply aggregation. The result could be saved to another destination. Apply Watermark to handle late data.\n",
    "agg_destination = \"/mnt/Blob/Vehicle_Agg\"\n",
    "\n",
    "kafkajsonDF.withWatermark(\"timestamp\",\"4 minutes\").groupBy(window('timestamp',\"1 minutes\"),'id').count().coalesce(1) \\\n",
    ".writeStream.format(\"delta\") \\\n",
    ".outputMode(\"complete\") \\\n",
    ".option(\"truncate\", \"false\") \\\n",
    ".option(\"checkpointLocation\", \"/mnt/Blob/Vehicle_Chkpoint1/\") \\\n",
    ".start(agg_destination) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data can thus be streamed to a Delta table \n",
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS delta_table\n",
    "USING DELTA\n",
    "LOCATION raw_destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code to read data as streaming data from the Delta table.\n",
    "display(spark.readStream.format(\"delta\").table(delta_table).groupBy(\"source\").count().orderBy(\"source\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and writing to Delta Tables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta Table actually can be thought of a bunch of snappy-compressed parquet files, with Delta Log. This offer the below benefits:\n",
    "- Easy rollback since it tracks every changes to the table in the delta log\n",
    "- ACID compliance\n",
    "- Enforce constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "ord_path = \"/mnt/Gen2/Orders/parquetFiles\"\n",
    "\n",
    "# Delta output path\n",
    "delta_path = \"/mnt/Gen2/Orders/delta\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save into Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input into dataframe\n",
    "df_ord = (spark.read.format(\"parquet\").load(ord_path)\n",
    "      .withColumn(\"timestamp\", current_timestamp())\n",
    "      .withColumn(\"O_OrderDateYear\", year(col(\"O_OrderDate\")))\n",
    "     )\n",
    "\n",
    "# Save into delta path. Go over to the ADLS Gen2 container and you should see new files got created in the delta path\n",
    "# Files are organized into different folders according to the \"partitionBy\" value\n",
    "df_ord.write.format(\"delta\").partitionBy(\"O_OrderDateYear\").mode(\"overwrite\").save(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or alternatively, this code would implicitly convert the Parquet files to delta format for you can create a Delta table pointing to that location\n",
    "# After running this you will see _delta_log created in the ord_path, then you can run the following command with LOCATION = ord_path\n",
    "%sql\n",
    "CONVERT TO DELTA parquet.`{ord_path}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Delta table. Visit the \"Data\" section in Databricks, you will see the relevant table\n",
    "%sql\n",
    "DROP TABLE IF EXISTS Orders;\n",
    "CREATE TABLE Orders\n",
    "USING DELTA\n",
    "LOCATION delta_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Via SQL\n",
    "%sql\n",
    "SELECT o.*\n",
    "FROM Orders o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Via Python\n",
    "deltaTable = spark.read.format(\"delta\").load(delta_path)\n",
    "deltaTable.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization\n",
    "- Combining multiple small files via [OPTIMIZE and ZORDER](https://www.confessionsofadataguy.com/exploring-delta-lakes-zorder-and-performance-on-databricks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
