{"cells": [{"cell_type": "code", "execution_count": 19, "id": "e0c7594c-1429-4f29-9e1e-e863ca9ba7ad", "metadata": {}, "outputs": [], "source": "import pyspark\nfrom pyspark.sql.types import StructField, StructType, StringType, LongType\nfrom pyspark.sql import SparkSession\nfrom py4j.protocol import Py4JJavaError"}, {"cell_type": "markdown", "id": "548ea0c7-fa8d-4f1a-8775-c6276b6e7b42", "metadata": {}, "source": "# Dataframe example\n- This is about reading data from BQ. Notice that when we load the data through the method described below, no query is issued via BQ.\n- Alternatively you can also read the data by issuing query to BQ directly, it will create a temp table under the materializationDataset, and thus be loaded into Spark for more information read https://github.com/GoogleCloudDataproc/spark-bigquery-connector."}, {"cell_type": "code", "execution_count": 20, "id": "251126f7-2963-479f-b50f-2cec1c18ff2d", "metadata": {}, "outputs": [{"data": {"text/plain": "pyspark.sql.dataframe.DataFrame"}, "execution_count": 20, "metadata": {}, "output_type": "execute_result"}], "source": "# Create a two column schema consisting of a string and a long integer\nschema = StructType(\n    [\n        StructField(\"subreddit\", StringType(), True),\n        StructField(\"count\", LongType(), True)\n    ]\n)\n\n# Create an empty DataFrame. We will continuously union our output with this\nsubreddit_counts = spark.createDataFrame([], schema)\n\n# By default, SparkSession will load the data as a DataFrame\ntable_df = spark.read.format('bigquery').option('table', \"fh-bigquery.reddit_posts.2017_01\").load()\ntype(table_df)"}, {"cell_type": "code", "execution_count": 21, "id": "56861332-eff1-4615-abcf-32ab7c915b46", "metadata": {}, "outputs": [], "source": "# Establish a set of years and months to iterate over\nyears = ['2017']\nmonths = ['01', '02']\n\n# Keep track of all tables accessed via the job\ntables_read = []\n\nfor year in years:\n    for month in months:\n\n        # In the form of <project-id>.<dataset>.<table>\n        table = f\"fh-bigquery.reddit_posts.{year}_{month}\"\n\n        # If the table doesn't exist we will simply continue and not\n        # log it into our \"tables_read\" list\n        try:\n            table_df = spark.read.format('bigquery').option('table', table).load()\n            tables_read.append(table)\n        except Py4JJavaError as e:\n            if f\"Table {table} not found\" in str(e):\n                continue\n            else:\n                raise\n\n        # We perform a group-by on subreddit, aggregating by the count and then\n        # unioning the output to our base dataframe\n        subreddit_counts = (\n            table_df\n            .groupBy(\"subreddit\")\n            .count()\n            .union(subreddit_counts)\n        )"}, {"cell_type": "code", "execution_count": 22, "id": "cf7e443f-44f8-4c83-b84a-c4217c2679d2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "The following list of tables will be accounted for in our analysis:\nfh-bigquery.reddit_posts.2017_01\nfh-bigquery.reddit_posts.2017_02\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 12:=============================================>            (7 + 2) / 9]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+------+\n|           subreddit| count|\n+--------------------+------+\n|           AskReddit|489877|\n|RocketLeagueExchange|394825|\n|          The_Donald|358716|\n|       AutoNewspaper|324971|\n|GlobalOffensiveTrade|224767|\n|                news|144650|\n|              videos|143173|\n|      Showerthoughts|138401|\n|               funny|132761|\n|              gaming| 97418|\n|           Overwatch| 96623|\n|            politics| 93481|\n|                 aww| 82689|\n|                pics| 79858|\n|     leagueoflegends| 78422|\n|        dirtykikpals| 75812|\n|              me_irl| 74686|\n| GlobalParadigmShift| 70842|\n|           worldnews| 67100|\n|        dirtypenpals| 64638|\n+--------------------+------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "print(\"The following list of tables will be accounted for in our analysis:\")\nfor table in tables_read:\n    print(table)\n\n# From our base table, we perform a group-by, summing over the counts.\n# We then rename the column and sort in descending order both for readability.\n# show() will collect the table into memory output the table to std out.\n(\n    subreddit_counts\n    .groupBy(\"subreddit\")\n    .sum(\"count\")\n    .withColumnRenamed(\"sum(count)\", \"count\")\n    .sort(\"count\", ascending=False)\n    .show()\n)"}, {"cell_type": "markdown", "id": "f034be0e-60b8-416a-a3d7-21871b22ca40", "metadata": {}, "source": "# RDD example\nThis is about reading a text file as unstructured data from GCS"}, {"cell_type": "code", "execution_count": 23, "id": "f774be3c-b76f-4ee1-8784-dc1f2f99d973", "metadata": {}, "outputs": [], "source": "inputUri='gs://pub/shakespeare/rose.txt'\noutputUri='gs://dataproc-staging-us-central1-712368347106-boh5iflc/misc/output_folder'"}, {"cell_type": "markdown", "id": "54c8fd0a-376c-4e93-aff9-3ac69e7bd8f7", "metadata": {}, "source": "SparkContext can be created from SparkSession"}, {"cell_type": "code", "execution_count": 24, "id": "08e684dd-a7b8-4335-9861-eb4a5271c961", "metadata": {}, "outputs": [], "source": "def read_text(inputUri, entrypoint):\n      \n    if entrypoint == sc:\n        lines = sc.textFile(inputUri)\n    elif entrypoint == spark:\n        lines = spark.sparkContext.textFile(inputUri)\n    \n    words = lines.flatMap(lambda line: line.split())\n    wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda count1, count2: count1 + count2)\n    return wordCounts\n\nwordCounts = read_text(inputUri, sc)"}, {"cell_type": "markdown", "id": "7f63cee5-a4d0-4134-9d28-3304d12d1c00", "metadata": {}, "source": "For RDD, the data must be collected to the driver with the collect() method for it to be meaningful. This will transform the data into list object"}, {"cell_type": "code", "execution_count": 25, "id": "355f20ff-7ec6-4118-8f32-3ca068c00c79", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 13:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "Printing wordCounts as RDD:\nPythonRDD[59] at RDD at PythonRDD.scala:53\n\nPrinting wordCounts after it as been collected to the driver:\n[('other', 1), ('name', 1), ('would', 1), ('smell', 1), ('as', 1), ('sweet.', 1), (\"What's\", 1), ('in', 1), ('name?', 1), ('That', 1), ('we', 1), ('call', 1), ('rose', 1), ('a', 2), ('which', 1), ('By', 1), ('any', 1)]\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "print(f\"Printing wordCounts as RDD:\\n{wordCounts}\\n\\nPrinting wordCounts after it as been collected to the driver:\\n{wordCounts.collect()}\")"}, {"cell_type": "markdown", "id": "62375ebd-b417-4541-bd3d-35b12972abe3", "metadata": {}, "source": "The RDD will be saved into multiple files"}, {"cell_type": "code", "execution_count": 28, "id": "6ff53d69-4eaf-4484-bd02-48dadd195998", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "<class 'pyspark.rdd.RDD'>\n"}], "source": "# Save\nwordCounts.saveAsTextFile(outputUri)\n\n# Read\nrdd_output = spark.sparkContext.textFile(outputUri)\nprint(type(rdd_output))"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.13"}}, "nbformat": 4, "nbformat_minor": 5}