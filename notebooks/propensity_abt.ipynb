{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # Rolling Window Features"]},{"cell_type":"markdown","metadata":{},"source":["Following notebook showcases an example workflow of creating rolling window features and building a model to predict which customers will buy in next 4 weeks.\n","\n","This uses dummy sales data but the idea can be implemented on actual sales data and can also be expanded to include other available data sources such as click-stream data, call center data, email contacts data, etc.\n","\n","***\n","\n","<b>Spark 3.1.2</b> (with Python 3.8) has been used for this notebook.<br>\n","Refer to [spark documentation](https://spark.apache.org/docs/3.1.2/api/sql/index.html) for help with <b>data ops functions</b>.<br>\n","Refer to [this article](https://medium.com/analytics-vidhya/installing-and-using-pyspark-on-windows-machine-59c2d64af76e) to <b>install and use PySpark on Windows machine</b>."]},{"cell_type":"markdown","metadata":{},"source":["### Building a spark session\n","To create a SparkSession, use the following builder pattern:\n"," \n","`spark = SparkSession\\\n","    .builder\\\n","    .master(\"local\")\\\n","    .appName(\"Word Count\")\\\n","    .config(\"spark.some.config.option\", \"some-value\")\\\n","    .getOrCreate()`"]},{"cell_type":"code","execution_count":1,"metadata":{"ExecuteTime":{"end_time":"2021-12-28T06:25:21.626967Z","start_time":"2021-12-28T06:25:21.318113Z"}},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F\n","from pyspark.sql import Window\n","from pyspark.sql.types import FloatType, BooleanType\n","import pandas as pd\n","import re\n","import datetime\n","import pytz"]},{"cell_type":"code","execution_count":2,"metadata":{"ExecuteTime":{"end_time":"2021-12-28T06:25:32.603308Z","start_time":"2021-12-28T06:25:31.992692Z"}},"outputs":[],"source":["spark = SparkSession\\\n","    .builder\\\n","    .appName(\"rolling_window\")\\\n","    .config(\"spark.executor.memory\", \"1536m\")\\\n","    .config(\"spark.driver.memory\", \"2g\")\\\n","    .getOrCreate()"]},{"cell_type":"code","execution_count":3,"metadata":{"ExecuteTime":{"end_time":"2021-12-28T06:25:34.292160Z","start_time":"2021-12-28T06:25:34.252288Z"}},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-3ef8-m.us-central1-f.c.adroit-hall-301111.internal:34369\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.3</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f36e0fe29a0>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"markdown","metadata":{},"source":["## Data prep\n","\n","We will be using window functions to compute relative features for all dates. We will first aggregate the data to customer x week level so it is easier to handle.\n","\n","<mark>The week level date that we create will serve as the 'reference date' from which everything will be relative.</mark>\n","\n","All the required dimension tables have to be joined with the sales table prior to aggregation so that we can create all required features."]},{"cell_type":"markdown","metadata":{},"source":["### Read input datasets"]},{"cell_type":"code","execution_count":4,"metadata":{"tags":[]},"outputs":[],"source":["class ABT():\n","    \n","    # TODO: categorical handling https://github.com/GoogleCloudPlatform/cloud-for-marketing/tree/main/marketing-analytics/predicting/ml-data-windowing-pipeline\n","    \n","    def __init__(self, source, user_col, date_col):\n","        \"\"\"\n","        IMPORTANT: Please make sure the date_col is loaded as date in the first place\n","        \"\"\"\n","        self.source = source\n","        self.user_col = user_col\n","        self.date_col = date_col\n","    \n","    def set_snapshot_freq(self, freq):\n","        \"\"\"\n","        1. Add column '_snapshot' to self.source to indicate aggregation level,\n","            e.g. if freq = 'W-SAT' then it will be converted to the Saturday of that week.\n","            '_snapshot_date' is also defined as self.snapshot_col\n","        2. Also add self.date_range for subsequent densify operation\n","        \n","        See acceptable arguments here https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases\n","        \"\"\" \n","        \n","        self.freq = freq\n","        self.snapshot_col = '_snapshot_date'\n","        \n","        if freq == 'W-SAT':\n","            self.source = self.source.withColumn(self.snapshot_col, F.col(self.date_col) + 7 - F.dayofweek(self.date_col))\n","        \n","        elif freq == 'D':\n","            self.source = self.source.withColumn(self.snapshot_col, F.col(self.date_col))\n","        \n","        # get global date range\n","        global_min_date, global_max_date = self.source.groupBy().agg(\n","            F.min(self.snapshot_col).alias('global_min_date'),\n","            F.max(self.snapshot_col).alias('global_max_date')\n","        ).collect()[0]\n","       \n","        # densify the global date range, and create a Spark DataFrame from it, \n","        dt_rng = pd.date_range(start=global_min_date, end=global_max_date, freq=self.freq)\n","        df_date = pd.DataFrame(dt_rng, columns=[self.snapshot_col])\n","        self.date_range = spark.createDataFrame(df_date)\n","        self.date_range = self.date_range.withColumn(self.snapshot_col,F.to_date(self.snapshot_col))\n","        \n","        # write an output containing user col and snapshot col only\n","        self.output = self.source.select(self.user_col, self.snapshot_col).dropDuplicates()\n","    \n","    def simple_agg(self, *aggs):\n","        \"\"\"\n","        Aggregate values WITHOUT categorical breakdown e.g. total sales as columns\n","        \"\"\"\n","        temp_agg = self.source.groupBy(self.user_col, self.snapshot_col).agg(*aggs)\n","        self.output = self.output.join(temp_agg, on=[self.user_col, self.snapshot_col], how='left')\n","    \n","    def pivot_agg(self, dimension, agg, formatting):\n","        \"\"\"\n","        Aggregate values with categorical breakdown e.g. adding sales by product category as columns\n","        \"\"\"\n","        prefix, suffix = formatting.split('%')\n","        \n","        # Add prefix & sufix to value e.g. cat_A_salesAmt, cat_D_salesAmt, etc., and also replace space with underscore        \n","        temp_agg = self.source.withColumn(dimension, F.concat(\n","            F.lit(prefix),\n","            F.regexp_replace(F.col(dimension),' ','_'),\n","            F.lit(suffix),\n","        ))\n","        temp_agg = temp_agg.groupBy([self.user_col, self.snapshot_col]).pivot(dimension).agg(agg)\n","        self.output = self.output.join(temp_agg, on=[self.user_col, self.snapshot_col], how='left')\n","    \n","    def densify(self):\n","        \"\"\"\n","        Filling in the missing dates/weeks\n","        \"\"\"\n","        \n","        # get the start and end of available data, by user\n","        df_cust = self.source.groupBy(self.user_col).agg(\n","            F.min(self.snapshot_col).alias('user_min_date'),\n","            F.max(self.snapshot_col).alias('user_max_date')\n","        )\n","    \n","        # cross join the customers with global date range\n","        df_base = df_cust.crossJoin(F.broadcast(self.date_range))\n","        \n","        # filter to keep only week_end since first week per customer\n","        df_base = df_base.where(F.col('_snapshot_date')>=F.col('user_min_date'))\n","        \n","        # drop the by-user min/max_week which are not useful. Now df_base contains only user_col and _snapshot_date\n","        df_base = df_base.drop(\"user_min_date\", \"user_max_date\")\n","\n","        self.output = df_base.join(self.output, on=[self.user_col, self.snapshot_col], how='left')\n","        \n","    def y_windowing(self, target, prediction_window, mode):\n","        \n","        self.prediction_window = prediction_window\n","        window = Window.partitionBy(self.user_col).orderBy(self.snapshot_col).rowsBetween(1, self.prediction_window)\n","\n","        if mode == 'classification':\n","            # working field to turn the target to 0/1\n","            self.output = self.output.withColumn('_binary_flag', F.when(F.col(target)>0,1).otherwise(0))\n","\n","            # window to aggregate the flag over next n periods\n","            self.output = self.output.withColumn('_y_variable', F.max('_binary_flag').over(window).cast(BooleanType()))            \n","            \n","            # remove the working field\n","            self.output = self.output.drop(\"_binary_flag\")\n","            \n","        elif mode == 'regression':\n","            # window to aggregate the metric over next n periods\n","            self.output = self.output.withColumn('_y_variable', F.sum(target).over(window))\n","    \n","    # TODO: MAX should not be applied on an aggregated basis\n","    def x_windowing(self, columns, function, lookback, suffix):\n","        \n","        self.lookback = lookback\n","    \n","        # perform aggregation\n","        for one_lookback in lookback:\n","            window = Window.partitionBy(self.user_col).orderBy(self.snapshot_col).rowsBetween(1 - one_lookback, Window.currentRow)\n","            for one_column in columns:\n","                self.output = self.output.withColumn(one_column + f'_{one_lookback}{suffix}', function(F.col(one_column)).over(window))\n","    \n","    def trim(self):\n","        \"\"\"\n","        Trim dataset based on prediction window and the maximum of lookback.\n","        Can only be performed after y_windowing and x_aggregation.\n","        \n","        Idea: To also remove rows where all X are NULL, but maybe a bad idea:\n","            e.g. customers who have not been active in the last 7 days might be interested to log in\n","        \"\"\"\n","        \n","        valid_start = self.date_range.collect()[max(self.lookback)-1][0]\n","        valid_end = self.date_range.collect()[-self.prediction_window-1][0]\n","        self.output = self.output.where((F.col(self.snapshot_col)>=valid_start) & (F.col(self.snapshot_col)<=valid_end))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["ga4 = spark.read.format('bigquery').option('table', \"adroit-hall-301111.demo.ga4_abt\").load()"]},{"cell_type":"markdown","metadata":{},"source":["Aggregate data per row"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["user_col, date_col = 'user_pseudo_id', 'event_date'\n","\n","abt2 = ABT(ga4, user_col, date_col)\n","abt2.set_snapshot_freq('D')\n","abt2.simple_agg(\n","    F.sum('item_revenue').alias('items_revenue'),\n","    F.max('purchase_revenue').alias('max_revenue'),\n",")\n","\n","abt2.pivot_agg('event_name', F.countDistinct('event_id'), '%_count')\n","abt2.pivot_agg('category', F.countDistinct(F.when((F.col('event_name') == 'session_start'), F.col('event_id'))), '%_sessions')\n","\n","# To-doRemove columns that has too few users and thus might not be worthwhile for prediction,\n","# remember to include the y variable in the \"not in\" clause\n","# for aggregation in [i for i in abt2.output.columns if i not in (user_col, date_col)]:\n","#    if F.countDistinct(F.when((F.col('event_name') == 'session_start')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["abt2.output = abt2.output.drop(\"null\") # In case some dimension values are null "]},{"cell_type":"markdown","metadata":{},"source":["Densify and apply windowing"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["abt2.densify()\n","abt2.y_windowing('items_revenue', 3, 'classification')\n","\n","x_to_agg = [i for i in abt2.output.columns if re.match('.*count|.*revenue|.*sessions', i)]\n","abt2.x_windowing(x_to_agg, F.sum, [4], 'w_sum')\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Truncation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["abt2.trim()\n","abt2.output = abt2.output.where((F.col('screen_view_count_3d_sum')>0))\n","for i in [i for i in abt2.output.columns if re.match(f'.*count$', i)]:\n","    abt2.output = abt2.output.drop(i)"]},{"cell_type":"markdown","metadata":{},"source":["Get proportion of features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["divide_list = ['login','screen_view','session_start']\n","suffix_list = ['3d_sum'] #,'7d_sum','14d_sum','28d_sum'\n","\n","for event in divide_list:\n","    for suffix in suffix_list:\n","        x_to_divide = [i for i in abt2.output.columns if re.match(f'.*_{event}_*{suffix}$', i)]\n","        for variant in x_to_divide:\n","            abt2.output = abt2.output.withColumn(f'{variant}_pct', (F.col(variant) / F.col(f\"{event}_count_{suffix}\")))\n","\n","abt2.output = abt2.output.fillna(0)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/06/13 10:17:53 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","                                                                                \r"]}],"source":["output_date = datetime.datetime.now(pytz.timezone('Asia/Hong_Kong')).strftime('%Y%m%d')\n","\n","# save scored output\n","abt2.output.write \\\n","  .format(\"bigquery\") \\\n","  .option(\"temporaryGcsBucket\",\"dataproc-staging-us-central1-712368347106-boh5iflc/temp\") \\\n","  .save(f\"demo.ga4abt_{output_date}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"368px"},"toc_section_display":true,"toc_window_display":true},"vscode":{"interpreter":{"hash":"ce22748e8bebf5a2be273a894549765c8c30b36b7393e39d528e5cead3b97804"}}},"nbformat":4,"nbformat_minor":4}
