{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions  import from_unixtime\n",
    "from pyspark.sql.functions  import to_date\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import to_json, struct\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and write"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read from ADLS-Gen2\n",
    "For practical purposes we use blob storage (ADLS Gen-2) as data source, here's an example of how to read files stored there\n",
    "\n",
    "First, create a ADLS-Gen2 storage account. Then, authenticate into the storage endpoint. To do so, these needs to be performed in Azure:\n",
    "1. **Application registration**: You will need to register an Azure Active Directory (AAD) application. On the Azure portal home page, search for \"Azure Active Directory\" &rarr; select App registrations &rarr; New registration.\n",
    "2. **Create secret to the application**: Click on \"Certificates & secrets\" under the Manage heading &rarr; add a new client secret &rarr; Copy the value\n",
    "3. **Grant ADLS-Gen2 access to the registered Application**: In the ADLS-Gen2 storage account, navigate to Access Control (IAM) &rarr; Add &rarr; Add role assignment &rarr; Role = Storage Blob Data Contributor; Assign access to = User, group, or service principal; Select = The registered Application\n",
    "\n",
    "IMPORTANT NOTE: Mounting is no longer a recommended practice, see [here](https://docs.databricks.com/dbfs/mounts.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount configuration\n",
    "storage_account = \"xxx\" # Name of the ADLS Gen2 Storage Account\n",
    "storage_container = \"xxx\" # Name of the ADLS Gen2 Storage Container\n",
    "mount_name = \"xxx\"\n",
    "\n",
    "# Authentication\n",
    "client_id = \"xxx\" # Obtained from (1) the registered Application -> Application (client) ID\n",
    "tenant_id = \"xxx\" # Obtained from (1) the registered Application -> Directory (tenant) ID\n",
    "client_secret = \"xxx\" # Obtained from (2) the registered Application -> Copied secret value\n",
    "\n",
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "           \"fs.azure.account.oauth2.client.id\": client_id,\n",
    "           \"fs.azure.account.oauth2.client.secret\": client_secret,\n",
    "           \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"}\n",
    "\n",
    "# Check if the mount already exists, and mount if not\n",
    "mounts = [str(i) for i in dbutils.fs.ls('/mnt/')]\n",
    "if any(f\"dbfs:/mnt/{mount_name}\" in i for i in mounts):\n",
    "    print(mount_name + \" has already been mounted\")\n",
    "else:\n",
    "    dbutils.fs.mount(\n",
    "    source = f\"abfss://{storage_container}@{storage_account}.dfs.core.windows.net\",\n",
    "    mount_point = f\"/mnt/{mount_name}\",\n",
    "    extra_configs = configs\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files in the ADLS-Gen2 can thus be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Orders.csv file in a Spark dataframe\n",
    "df_ord= spark.read.format(\"csv\").option(\"header\",True).load(f\"abfss://staging_container@{storageEndPoint}/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing the below will create an External table in Databricks for you to read the CSV.\n",
    "# This is not about creating Delta tables however\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE Orders\n",
    "    {{(schema)}}\n",
    "USING {{csv}}\n",
    "OPTIONS (\n",
    "    path 'abfss://staging_container@{storageEndPoint}/orders',\n",
    "    header 'true',\n",
    "    delimiter ','\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to Delta Tables "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta Table is actually a bunch of snappy-compressed parquet files, with Delta Log files. It offer the following benefits:\n",
    "- Easy rollback since it tracks every changes to the table in the delta log\n",
    "- ACID compliance\n",
    "- Enforce constraint defined in DDL\n",
    "- Optimized performance\n",
    "\n",
    "Below is an example for creating Delta Table using CSV files, These will happen after executing the below:\n",
    "- Under the defined LOCATION, snappy-compressed parquet files will be created, alongside the _delta_log folder\n",
    "- You will find the relevant table under the \"Data\" section in Databricks\n",
    "- The table information will persist in Databricks's metastore even after you shut down the clusters\n",
    "\n",
    "Some notes to the code\n",
    "- The Temp View is necessary to allow Databricks to parse the CSV first, before making it a Delta table\n",
    "- LOCATION flag is used to make the table an unmanaged one, whose data does not reside in dfbs but in the specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for the raw data blobs\n",
    "original_path = f\"abfss://staging_container@{storageEndPoint}/orders\"\n",
    "\n",
    "# Delta output path\n",
    "delta_path = f\"abfss://loading_container@{storageEndPoint}/orders\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW Orders_vw\n",
    "    {{(schema, if necessary)}}\n",
    "USING {{file format e.g. csv}}\n",
    "OPTIONS (\n",
    "    path '{original_path}',\n",
    "    header 'true',\n",
    "    delimiter '|'\n",
    "    );\n",
    "\n",
    "CREATE OR REPLACE TABLE Orders\n",
    "LOCATION '{delta_path}'\n",
    "AS\n",
    "SELECT *, EXTRACT(YEAR FROM Event_Date) FROM Orders_vw\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For self-describing file formats like parquet, the syntax can be made easier. Note: This does not support manual schema declaration, like one would do for CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This will not work for CSV, beecause the CTAS statement cannot infer schema correctly\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE Orders\n",
    "LOCATION '{delta_path}'\n",
    "AS\n",
    "    SELECT *\n",
    "    FROM parquet.`{original_path}`\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative code using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input into dataframe\n",
    "df_ord = (spark.read.format(\"parquet\").load(original_path)\n",
    "      .withColumn(\"timestamp\", current_timestamp())\n",
    "      .withColumn(\"O_OrderDateYear\", year(col(\"O_OrderDate\")))\n",
    "     )\n",
    "\n",
    "# Save into delta path. Go over to the ADLS Gen2 container and you should see new files got created in the delta path\n",
    "# Files are organized into different folders according to the \"partitionBy\" value\n",
    "df_ord.write.format(\"delta\").partitionBy(\"O_OrderDateYear\").mode(\"overwrite\").option(\"path\",delta_path).saveAsTable(\"Orders\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creation, the table can be queried in SQL or Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Via SQL\n",
    "%sql\n",
    "SELECT o.*\n",
    "FROM Orders o\n",
    "\n",
    "# Via Python\n",
    "deltaTable = spark.read.format(\"delta\").table('Orders')\n",
    "deltaTable.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "# View history of the Delta table\n",
    "DESCRIBE HISTORY Orders\n",
    "\n",
    "# Query a specific version\n",
    "SELECT * FROM Orders VERSION AS OF 1\n",
    "\n",
    "# Restore a previous version\n",
    "RESTORE TABLE Orders VERSION AS OF 5\n",
    "\n",
    "# See important metadata about the table e.g. schema, database, location, etc.\n",
    "DESCRIBE EXTENDED Orders\n",
    "\n",
    "# View underlying file structure of the table e.g. number of files, partitioning, etc.\n",
    "DESCRIBE DETAIL Orders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Schema with Generated Columns\n",
    "\n",
    "As noted previously, CTAS statements do not support schema declaration. We note above that the timestamp column appears to be some variant of a Unix timestamp, which may not be the most useful for our analysts to derive insights. This is a situation where generated columns would be beneficial.\n",
    "\n",
    "Generated columns are a special type of column whose values are automatically generated based on a user-specified function over other columns in the Delta table (introduced in DBR 8.3).\n",
    "\n",
    "The code below demonstrates creating a new table while:\n",
    "1. Specifying column names and types\n",
    "1. Adding a <a href=\"https://docs.databricks.com/delta/delta-batch.html#deltausegeneratedcolumns\" target=\"_blank\">generated column</a> to calculate the date\n",
    "1. Providing a descriptive column comment for the generated column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE purchase_dates (\n",
    "  id STRING, \n",
    "  transaction_timestamp STRING, \n",
    "  price STRING,\n",
    "  date DATE GENERATED ALWAYS AS (\n",
    "    cast(cast(transaction_timestamp/1e6 AS TIMESTAMP) AS DATE))\n",
    "    COMMENT \"generated based on `transactions_timestamp` column\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because **`date`** is a generated column, if we write to **`purchase_dates`** without providing values for the **`date`** column, Delta Lake automatically computes them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More about Tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables vs Views vs CTE\n",
    "\n",
    "- Table\n",
    "    - Managed table: Data is actually stored in DBFS\n",
    "    - Unmanaged table: Data is stored in elsewhere e.g. ADLS-Gen2. Databricks only manages the metadata of the table\n",
    "- Views\n",
    "    - View: Will persist like table\n",
    "    - Temp View: Persist in the current notebook session only\n",
    "    - Global Temp View: Can be shared across different notebook sessions, until cluster restarts\n",
    "- CTE\n",
    "    - CTE: Referenced within the scope of a SQL statement only\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Constraint\n",
    "\n",
    "Because Delta Lake enforces schema on write, Databricks can support standard SQL constraint management clauses to ensure the quality and integrity of data added to a table.\n",
    "\n",
    "Databricks currently support two types of constraints:\n",
    "* <a href=\"https://docs.databricks.com/delta/delta-constraints.html#not-null-constraint\" target=\"_blank\">**`NOT NULL`** constraints</a>\n",
    "* <a href=\"https://docs.databricks.com/delta/delta-constraints.html#check-constraint\" target=\"_blank\">**`CHECK`** constraints</a>\n",
    "\n",
    "In both cases, you must ensure that no data violating the constraint is already in the table prior to defining the constraint. Once a constraint has been added to a table, data violating the constraint will result in write failure.\n",
    "\n",
    "Below, we'll add a **`CHECK`** constraint to the **`date`** column of our table. Note that **`CHECK`** constraints look like standard **`WHERE`** clauses you might use to filter a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "ALTER TABLE purchase_dates ADD CONSTRAINT valid_date CHECK (date > '2020-01-01');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrich Tables with Additional Options and Metadata\n",
    "\n",
    "So far we've only scratched the surface as far as the options for enriching Delta Lake tables.\n",
    "\n",
    "Below, we show evolving a CTAS statement to include a number of additional configurations and metadata.\n",
    "\n",
    "Our **`SELECT`** clause leverages two built-in Spark SQL commands useful for file ingestion:\n",
    "* **`current_timestamp()`** records the timestamp when the logic is executed\n",
    "* **`input_file_name()`** records the source data file for each record in the table\n",
    "\n",
    "We also include logic to create a new date column derived from timestamp data in the source.\n",
    "\n",
    "The **`CREATE TABLE`** clause contains several options:\n",
    "* A **`COMMENT`** is added to allow for easier discovery of table contents\n",
    "* A **`LOCATION`** is specified, which will result in an external (rather than managed) table\n",
    "* The table is **`PARTITIONED BY`** a date column; this means that the data from each data will exist within its own directory in the target storage location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE users_pii\n",
    "COMMENT \"Contains PII\"\n",
    "LOCATION \"${da.paths.working_dir}/tmp/users_pii\"\n",
    "PARTITIONED BY (first_touch_date)\n",
    "AS\n",
    "  SELECT *, \n",
    "    cast(cast(user_first_touch_timestamp/1e6 AS TIMESTAMP) AS DATE) first_touch_date, \n",
    "    current_timestamp() updated,\n",
    "    input_file_name() source_file\n",
    "  FROM parquet.`${da.paths.datasets}/ecommerce/raw/users-historical/`;\n",
    "  \n",
    "SELECT * FROM users_pii;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note about **`PARTITIONED BY`** in the code above:\n",
    "- Partitioning is not a recommended practice for Delta Tables. Most Delta Lake tables (especially small-to-medium sized data) will not benefit from partitioning. Because partitioning physically separates data files, this approach can result in a small files problem and prevent file compaction and efficient data skipping. Most tables with less than 1 TB of data do not require partitions, all partitions should contain at least 1 GB of data. \n",
    "- Do note that it is PARTITION`ED`\n",
    "- Partitioning can be updated via the `CREATE OR REPLACE TABLE` clause, which would create new folders to store the data partitioned by the new criteria. Contents in the old folders storing the old partitioned data are not removed unless you execute `VACCUM`. After this is executed, contents in the old folders are removed, but the folders themselves will remain\n",
    "\n",
    "**As a best practice, you should default to non-partitioned tables for most use cases when working with Delta Lake.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloning Delta Lake Tables\n",
    "Delta Lake has two options for efficiently copying Delta Lake tables.\n",
    "\n",
    "**`DEEP CLONE`** fully copies data and metadata from a source table to a target. This copy occurs incrementally, so executing this command again can sync changes from the source to the target location.\n",
    "\n",
    "If you wish to create a copy of a table quickly to test out applying changes without the risk of modifying the current table, **`SHALLOW CLONE`** can be a good option. Shallow clones just copy the Delta transaction logs, meaning that the data doesn't move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE purchases_clone\n",
    "DEEP/SHALLOW CLONE purchases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading into Delta Lake"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`CREATE OR REPLACE TABLE`** (CRAS):\n",
    "- Fully replace the contents of a table each time they execute.\n",
    "\n",
    "**`INSERT OVERWRITE`**:\n",
    "- Can only overwrite an existing table, not create a new one like our CRAS statement\n",
    "- Can overwrite only with new records that match the current table schema -- and thus can be a \"safer\" technique for overwriting an existing table without disrupting downstream consumers\n",
    "- Can overwrite individual partitions\n",
    "\n",
    "**`INSERT INTO SELECT XXX`**:\n",
    "- Self-explanatory\n",
    "\n",
    "**`MERGE INTO a USING b`**:\n",
    "- Self-explanatory\n",
    "\n",
    "**`COPY INTO`**:\n",
    "- An idempotent option to incrementally ingest data from external systems.\n",
    "COPY INTO sales\n",
    "FROM \"path\"\n",
    "FILEFORMAT = PARQUET"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "- Remove unused files from a table directory via [VACCUM](https://learn.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/language-manual/delta-vacuum/), add `DRY RUN` to preview previous versions to be deleted first. This is especially important for deleting PII information, as after DELETE the PII might still exist in previous images of the data\n",
    "- Compact small files via ```OPTIMIZE```\n",
    "- Indexing\n",
    "  - **Z-Order**: Collocate related information for high cardinality data, often most effective when working with queries that filter against continuous numeric variables. ```ZORDER BY (comma separated column names)```\n",
    "  - **Bloom Filter**: Probabilistically identifying files that may contain data using fields containing arbitrary text. ```CREATE BLOOMFILTER INDEX ON TABLE table_name FOR COLUMNS(indexed_col OPTIONS (fpp=0.1, numItems=200))```\n",
    "- Turning on Auto Optimize and Auto Compaction help us avoid the tables containing too many small files. For more information on these settings, see [here](https://docs.databricks.com/delta/optimizations/auto-optimize.html)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming data pipeline from EventHub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of building the Bronze, Silver, and Gold Zone for a streaming data pipeline\n",
    "- Bronze: Read live data from EventHub, and historical data from ADLS-Gen2. Parse content and union them\n",
    "- Silver: Implement business rules and data cleansing process and join with lookup tables\n",
    "- Gold: Data is aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "db_name = \"VehicleSensor\"\n",
    "\n",
    "def get_config(zone):\n",
    "    return {\n",
    "    'delta_path': f\"/mnt/SensorData/vehiclestreamingdata/{zone}/delta\",\n",
    "    'chkpt_path': f\"/mnt/SensorData/vehiclestreamingdata/{zone}/chkpt\",\n",
    "    'schema_path': f\"/mnt/SensorData/vehiclestreamingdata/{zone}/schema\",\n",
    "    'delta_table': f\"VehicleDelta_{zone}\"\n",
    "    }\n",
    "\n",
    "# Create DB first\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS{db_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronze Zone"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming from Kafka"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spark DataFrame which reads from the Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC = \"cookbook-eventhub\" # Event Hub namespace\n",
    "BOOTSTRAP_SERVERS = \"cookbook-eventhub.servicebus.windows.net:9093\" # Host name of Event Hub:9093, 9093 is the port for Kafka\n",
    "\n",
    "# Go to Event Hub's Shared Access Policies -> Click onto a policy -> Copy Connection stringâ€“primary key here\n",
    "CONN_STRING = \"Endpoint=sb://kafkaenabledeventhubns.servicebus.windows.net/;SharedAccessKeyName=sendreceivekafka;SharedAccessKey=4vgxasdsdasd4aVcUWBvYp44sdasaasasasasasasvoVE=\" \n",
    "\n",
    "# The $ConnectionString and $Default are fixed values, don't update them\n",
    "EH_SASL = f\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"$ConnectionString\\\" password=\\\"{CONN_STRING}\\\";\"\n",
    "GROUP_ID = \"$Default\" \n",
    "\n",
    "# // Read stream using Spark SQL (structured streaming)\n",
    "# // consider adding .option(\"startingOffsets\", \"earliest\") to read earliest available offset during testing\n",
    "kafkaDF = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"subscribe\", TOPIC) \\\n",
    "    .option(\"kafka.bootstrap.servers\", BOOTSTRAP_SERVERS) \\\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n",
    "    .option(\"kafka.sasl.jaas.config\", EH_SASL) \\\n",
    "    .option(\"kafka.request.timeout.ms\", \"60000\") \\\n",
    "    .option(\"kafka.session.timeout.ms\", \"60000\") \\\n",
    "    .option(\"kafka.group.id\", \"POC\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load() \\\n",
    "    .withColumn(\"source\", lit(TOPIC)) # Optional: Also add the topic as column\n",
    "\n",
    "#Check if streaming is on and getting the schema for the kakfa dataframe \n",
    "print(kafkaDF.isStreaming)\n",
    "print(kafkaDF.printSchema())\n",
    "\n",
    "#It should then output something like this:\n",
    "#\n",
    "#True\n",
    "#root\n",
    "# |-- key: binary (nullable = true)\n",
    "# |-- value: binary (nullable = true)\n",
    "# |-- topic: string (nullable = true)\n",
    "# |-- partition: integer (nullable = true)\n",
    "# |-- offset: long (nullable = true)\n",
    "# |-- timestamp: timestamp (nullable = true)\n",
    "# |-- timestampType: integer (nullable = true)\n",
    "# |-- source: string (nullable = true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the Kafka message and writing the streaming data to Delta table. Some important configurations for ```writeStream```:\n",
    "- ```trigger``` is for controlling the frequency of writes, default is fixed interval micro-batches of 500ms. This could be modified with ```.trigger(processingTime='10 seconds')```. For incremental batch jobs, use ```.trigger(availableNow=True)```\n",
    "\n",
    "There are a few more configurations under `options`:\n",
    "- ```checkpointLocation``` is set so that it can recover from failure in the event of server failure. If this is not defined, all state data around the streaming job is lost, and upon restart, the job must start from scratch. Each query must have a different checkpoint location.\n",
    "- ```maxFiles[Bytes]PerTrigger```: Maintain a consistent batch size and prevents large batches from leading to spill and cascading micro-batch processing delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the schema for the 'value' field in Kafka message\n",
    "jsonschema = StructType() \\\n",
    "      .add(\"id\", StringType()) \\\n",
    "      .add(\"eventtime\", TimestampType()) \\\n",
    "      .add(\"rpm\", IntegerType()) \\\n",
    "      .add(\"speed\", IntegerType()) \\\n",
    "      .add(\"kms\", IntegerType()) \\\n",
    "      .add(\"lfi\", IntegerType())  \\\n",
    "      .add(\"lat\", DoubleType()) \\\n",
    "      .add(\"long\", DoubleType())\n",
    "\n",
    "# Parse data and create writeStream\n",
    "kafkaDF.selectExpr(\"CAST(key AS STRING) as key\", \"CAST(value AS STRING) as value\", \"source\") \\\n",
    "            .withColumn('vehiclejson', from_json(col('value'), jsonschema)) \\\n",
    "            .select(\"key\", \"value\", \"source\", \"vehiclejson.*\") \\\n",
    "            .writeStream.format(\"delta\") \\\n",
    "            .trigger(processingTime = \"2 minutes\") \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .option(\"checkpointLocation\",get_config(\"Bronze\")['chkpt_path']) \\\n",
    "            .start(get_config(\"Bronze\")['delta_path']) \n",
    "\n",
    "# Data can thus be streamed to a Delta table \n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {db_name}.{get_config(\"Bronze\")['delta_table']}\n",
    "LOCATION '{get_config(\"Bronze\")['delta_path']}'\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming from ADLS-Gen2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you have historical data in ADLS-Gen2, and you would like to stream-read the data as files are ingested there, AutoLoader is the prefered method. This method is also useful for batch jobs, as likely should be the case here.\n",
    "\n",
    "Some important options for Auto Loader readStream:\n",
    "- ```cloudFiles.maxFiles[Bytes]PerTrigger```: Same configuration as described above, but set using this key for Auto Loader\n",
    "- ```cloudFiles.schemaLocation```: The location to store the inferred schema and subsequent changes, to deal with schema evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from ADLS Gen-2 using readStream API and writing as delta format\n",
    "def append_batch_source(data_source_path, value_schema):\n",
    "  \n",
    "  kafkaDF = (spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"parquet\")\n",
    "    .option(\"cloudFiles.maxBytesPerTrigger\", \"parquet\")\n",
    "    .option(\"cloudFiles.schemaLocation\", get_config(\"Hist\")['schema_path'])\n",
    "    .load(data_source_path)\n",
    "  )\n",
    "\n",
    "  parsedDF=kafkaDF.withColumn(\"source\", lit('historical')) \\\n",
    "            .selectExpr(\"CAST(key AS STRING) as key\", \"CAST(value AS STRING) as value\", \"source\") \\\n",
    "            .withColumn('vehiclejson', from_json(col('value'), value_schema)) \\\n",
    "            .select(\"key\", \"value\", \"source\", \"vehiclejson.*\") \\\n",
    "            .writeStream.format(\"delta\") \\\n",
    "            .trigger(availableNow=True) \\\n",
    "            .option(\"checkpointLocation\",get_config(\"Hist\")['chkpt_path']) \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .start(get_config(\"Hist\")['delta_path']) \n",
    "\n",
    "  return parsedDF\n",
    "\n",
    "# Create historical delta table\n",
    "append_batch_source('/xxx', jsonschema)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {db_name}.{get_config(\"Historical\")['delta_table']}\n",
    "LOCATION '{get_config(\"Historical\")['delta_path']}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect schema\n",
    "spark.sql(f\"\"\"\n",
    "DESCRIBE FORMATTED {db_name}.{get_config(\"Bronze\")['delta_table']}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union the live and historical data, and generate Temp View"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With live and historical delta tables created, we can union them for subsequent usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Streaming Data from Bronze and Historical tables\n",
    "df_bronze=spark.readStream.format(\"delta\").option(\"latestFirst\", \"true\").table(f\"{db_name}.{get_config(\"Bronze\")['delta_table']}\")\n",
    "df_historical=spark.readStream.format(\"delta\").option(\"latestFirst\", \"true\").table(f\"{db_name}.{get_config(\"Historical\")['delta_table']}\")\n",
    "\n",
    "#Joining both historical and Bronze Streaming Data. The TempView can be used like CTE in SQL statements\n",
    "df_bronze_all = df_bronze.union(df_historical)\n",
    "df_bronze_all.createOrReplaceTempView(\"vw_TempBronzeAll\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver Zone"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Azure SQL DB for lookup tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish connection to Azure SQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config details for Azure SQL DB for VehicleInformation and LocationInformation tables\n",
    "sqldbusername = dbutils.secrets.get(scope=\"KeyVaultScope\",key=\"VehicleInformationDBUserId\")\n",
    "sqldbpwd = dbutils.secrets.get(scope=\"KeyVaultScope\",key=\"VehicleInformationDBPwd\")\n",
    "\n",
    "jdbcHostname = \"vehicledemoinformatiosrvr.database.windows.net\"\n",
    "jdbcDatabase = \"VehicleInformationDB\"\n",
    "jdbcPort = 1433\n",
    "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2};user={3};password={4}\".format(jdbcHostname, jdbcPort, jdbcDatabase, sqldbusername, sqldbpwd)\n",
    "connectionProperties = {\n",
    "  \"user\" : sqldbusername,\n",
    "  \"password\" : sqldbpwd,\n",
    "  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the two lookup tables and create Temp Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading dbo.VehicleInfo master table from Azure SQL DB and creating a view\n",
    "vehicleInfo = \"(select VehicleId,Make,Model,Category,ModelYear from dbo.VehicleInformation) vehicle\"\n",
    "df_vehicleInfo = spark.read.jdbc(url=jdbcUrl, table=vehicleInfo, properties=connectionProperties)\n",
    "df_vehicleInfo.createOrReplaceTempView(\"vw_VehicleMaster\")\n",
    "display(df_vehicleInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading dbo.LocationInfo master table from Azure SQL DB and creating a view\n",
    "locationInfo = \"(select Borough,Location,Latitude,Longitude from dbo.LocationInfo) vehicle\"\n",
    "df_locationInfo = spark.read.jdbc(url=jdbcUrl, table=locationInfo, properties=connectionProperties)\n",
    "df_locationInfo.createOrReplaceTempView(\"vw_LocationMaster\")\n",
    "display(df_locationInfo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-duplication"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Kafka provides at-least-once guarantees on data delivery, all Kafka consumers should be prepared to handle duplicate reocrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped_df = (spark.readStream\n",
    "                   .table(\"vw_TempBronzeAll\")\n",
    "                   .select(\"*\")\n",
    "                   .withWatermark(\"time\", \"30 seconds\")\n",
    "                   .dropDuplicates([\"id\", \"eventtime\"])\n",
    "                   .createOrReplaceTempView(\"vw_TempBronzeAll_dedup\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the deduplication, we are ready to insert to records into Silver. Because we're using SQL to write to our Delta table, we'll need to make sure this table exists before we begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {get_config(\"Silver\")['delta_table']}\n",
    "(id STRING, eventtime TIMESTAMP, rpm INTEGER, Year INTEGER, Month INTEGER, Day INTEGER, Hour INTEGER, \n",
    "Make STRING, Model STRING, Category STRING, Borough STRING, Location STRING)\n",
    "USING DELTA\n",
    "LOCATION {get_config(\"Silver\")['delta_path']}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta Lake has optimized functionality for insert-only merges. This operation is ideal for de-duplication: define logic to match on unique keys, and only insert those records for keys that don't already exist.\n",
    "\n",
    "Note that in this application, we proceed in this fashion because we know two records with the same matching keys represent the same information. If the later arriving records indicated a necessary change to an existing record, we would need to change our logic to include a **`WHEN MATCHED`** clause.\n",
    "\n",
    "A merge into query is defined in SQL below against a view titled **`stream_updates`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = f\"\"\"\n",
    "  MERGE INTO {get_config(\"Silver\")['delta_table']} a\n",
    "  USING (SELECT\n",
    "    stream_updates.id\n",
    "    ,stream_updates.eventtime\n",
    "    ,stream_updates.rpm\n",
    "    ,Year(eventtime) as Year\n",
    "    ,month(eventtime) as Month\n",
    "    ,day(eventtime) as Day\n",
    "    ,hour(eventtime) as Hour\n",
    "    ,m.Make\n",
    "    ,m.Model\n",
    "    ,m.Category\n",
    "    ,l.Borough\n",
    "    ,l.Location\n",
    "    FROM stream_updates\n",
    "    LEFT JOIN vw_VehicleMaster m on stream_updates.id = m.VehicleId\n",
    "    LEFT join vw_LocationMaster l on stream_updates.lat = l.Latitude and stream_updates.long = l.Longitude\n",
    "    ) b\n",
    "  ON a.VehicleId=b.VehicleId AND a.eventtime=b.eventtime\n",
    "  WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark Structured Streaming **`foreachBatch`** method allows users to define custom logic when writing.\n",
    "\n",
    "The logic applied during **`foreachBatch`** addresses the present microbatch as if it were a batch (rather than streaming) data, thus enabling the use of some functions, like merge, or window functions. Applying these functions without **`foreachBatch`** would return error, see the end of this section\n",
    "\n",
    "The function called in **`foreachBatch`** requires two parameters: a DataFrame or Dataset that has the output data of a micro-batch and the unique ID of the micro-batch. The following cell defines simple logic that will allow us to register any SQL **`MERGE INTO`** query for use in a Structured Streaming write. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_to_delta(microBatchDF, batchId):\n",
    "    microBatchDF.createOrReplaceTempView(\"stream_updates\")\n",
    "    microBatchDF._jdf.sparkSession().sql(sql_query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform the upsert by using the previously defined function in our **`foreachBatch`** logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver = (deduped_df.writeStream\n",
    "            .format(\"delta\") \\\n",
    "            .foreachBatch(upsert_to_delta)\n",
    "            .outputMode(\"update\") \\\n",
    "            .option(\"checkpointLocation\",get_config(\"Silver\")['chkpt_path'])  \\\n",
    "            .start()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to read data as streaming data from the Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.readStream.format(\"delta\").table(get_config(\"Silver\")['delta_table']).groupBy(\"Make\").count().orderBy(\"Make\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If we try to apply this to a streaming read of our data without using ```foreachBatch```, we'll learn that\n",
    "> Non-time-based windows are not supported on streaming DataFrames\n",
    "\n",
    "Below is an example for an attempt to perform a window-function based deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranked_df = (spark.readStream\n",
    "#                   .table(\"bronze\")\n",
    "#                   .filter(\"topic = 'user_info'\")\n",
    "#                   .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "#                   .select(\"v.*\")\n",
    "#                   .filter(F.col(\"update_type\").isin([\"new\", \"update\"]))\n",
    "#                   .withColumn(\"rank\", F.rank().over(window))\n",
    "#                   .filter(\"rank == 1\").drop(\"rank\"))\n",
    "#\n",
    "# display(ranked_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality enforcement "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be implemented by:\n",
    "1. Enforcing **`Table Constraints`**, and sending the unconforming data to a Delta table\n",
    "2. Adding a CASE WHEN flag to the Silver table itself\n",
    "\n",
    "Below is a demonstration of #1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table constraints apply boolean filters to columns within a table and prevent data that does not fulfill these constraints from being written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "ALTER TABLE heart_rate_silver ADD CONSTRAINT validbpm CHECK (heartrate > 0);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a table to store quarantined records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS bpm_quarantine\n",
    "    (device_id LONG, time TIMESTAMP, heartrate DOUBLE)\n",
    "USING DELTA\n",
    "LOCATION '${da.paths.user_db}/bpm_quarantine'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Structured Streaming operations, writing to an additional table can be accomplished within **`foreachBatch`** logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "MERGE INTO heart_rate_silver a\n",
    "USING stream_updates b\n",
    "ON a.device_id=b.device_id AND a.time=b.time\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\"\n",
    "\n",
    "class Upsert:\n",
    "    def __init__(self, query, update_temp=\"stream_updates\"):\n",
    "        self.query = query\n",
    "        self.update_temp = update_temp \n",
    "        \n",
    "    def upsert_to_delta(self, micro_batch_df, batch):\n",
    "        micro_batch_df.filter(\"heartrate\" > 0).createOrReplaceTempView(self.update_temp)\n",
    "        micro_batch_df._jdf.sparkSession().sql(self.query)\n",
    "        # Notice the .write here\n",
    "        micro_batch_df.filter(\"heartrate\" <= 0).write.format(\"delta\").mode(\"append\").saveAsTable(\"bpm_quarantine\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that within the **`foreachBatch`** logic, the DataFrame operations are treating the data in each batch as if it's static rather than streaming.\n",
    "\n",
    "As such, we use the **`write`** syntax instead of **`writeStream`**.\n",
    "\n",
    "This also means that our exactly-once guarantees are relaxed. In our example above, we have two ACID transactions:\n",
    "1. Our SQL query executes to run an insert-only merge to avoid writing duplicate records to our silver table.\n",
    "2. We write a microbatch of records with negative heartrates to the **`bpm_quarantine`** table\n",
    "\n",
    "If our job fails after our first transaction completes but before the second completes, we will re-execute the full microbatch logic on job restart.\n",
    "\n",
    "However, because our insert-only merge already prevents duplicate records from being saved to our table, this will not result in any data corruption."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Zone"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the data and write to Gold Delta table. Important concepts include the below:\n",
    "\n",
    "readStream:\n",
    "- ```withWatermark```: Control the threshold for how long to continue processing updates for a given state entity (e.g. count).\n",
    "- ```window```: Defines the aggregation window for streaming data. Options include:\n",
    "  - **Tumbling**: Non-overlapping windows with fixed length ```window('eventtime','1 hour')```\n",
    "  - **Sliding**: Overlapping windows with fixed length, which emits result with between a fixed interval ```window('eventtime','1 hour', '5 minutes')```\n",
    "  - **Session**: Window is of dynamic length, which expires after x minutes without incoming event ```session_window(\"eventTime\", \"5 minutes\")```\n",
    "\n",
    "\n",
    "writeStream:\n",
    "- ```outputMode```: How the processed data is pushed to sink. 3 modes are available: ```append``` (the default), ```complete``` (this mode is used only when you have streaming aggregated data), and ```update``` (just outputs the updated aggregated results every time to data sink when new data arrives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define readStream and writeStream\n",
    "df_gold=(\n",
    "spark.readStream.format(\"delta\").option(\"latestFirst\", \"true\").table(f\"{db_name}.{get_config(\"Silver\")['delta_table']}\")\n",
    "    .withWatermark(\"timestamp\",\"4 minutes\")\n",
    "    .groupBy(window(\"eventtime\",\"1 hour\"),\"Make\",\"Borough\",\"Location\",\"Month\",\"Day\",\"Hour\").count()) \\\n",
    "        .writeStream.format(\"delta\") \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .option(\"checkpointLocation\", get_config(\"Gold\")['chkpt_path']) \\\n",
    "        .start(get_config(\"Gold\")['delta_path'])\n",
    "\n",
    "# Create Delta table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {db_name}.{get_config(\"Gold\")['delta_table']}\n",
    "LOCATION '{get_config(\"Gold\")['delta_path']}'\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Live Tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta Live Tables allows Python/SQL statements to be structured as ELT pipelines.\n",
    "\n",
    "Comparison between Workflows and DLT:\n",
    "- **DLT**: More for ELT jobs in batch/streaming mode, with built-in data quality constraints, and monitorng & logging. \n",
    "- **Workflows**: Can orchestrate arbitrary codes and ML tasks. DLT can be a task under Workflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings\n",
    "\n",
    "**Storage location**: Where the output tables and metadata required for processing will be stored. This location is either DBFS or another location you provide. Under the specified location, these folders storing Delta Lake tables are created:\n",
    "-  ```autoloader/``` and ```checkpoints/```: Directories contain data used to manage incremental data processing with Structured Streaming.\n",
    "- ```system/```: The ```events/``` subfolder captures events associated with the pipeline e.g. INITIALIZING, QUEUED, etc, lineage and data quality metrics are also available\n",
    "- ```tables/```: The tables themselves, each subfolder contains a Delta Lake table being managed by DLT.\n",
    "\n",
    "**Target**:\n",
    "- The name of a database for persisting pipeline output data.\n",
    "\n",
    "**Configuration**:\n",
    "- Define variables for parameterization, see [here](https://learn.microsoft.com/en-us/azure/databricks/workflows/delta-live-tables/delta-live-tables-configuration#parameterize-pipelines)\n",
    "\n",
    "**Pipeline Mode**: Specifies how the pipeline will be run.\n",
    "- **Triggered** pipelines run once and then shut down until the next manual or scheduled update.\n",
    "- **Continuous** pipelines run continuously, ingesting new data as it arrives."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python vs SQL\n",
    "| Python | SQL | Notes |\n",
    "|--------|--------|--------|\n",
    "| Python API | Proprietary SQL API |  |\n",
    "| no dlt module, no syntax check | has syntax checks| In Python, if you run a DLT notebook cell on its own it will show in error, whereas in SQL it will check if the command is syntactically valid and tell you. In both cases, individual notebook cells are not supposed to be run for DLT pipelines. |\n",
    "| A Note on Imports | None | The dlt module should be explicitly imported into your Python notebook libraries. In SQL, this is not the case. |\n",
    "| Tables as DataFrames | Tables as Query Results | The Python DataFrame API allows for multiple transformations of a dataset by stringing multiple API calls together. Compared to SQL, those same transformations must be saved in temporary tables as they are transformed. |\n",
    "|@dlt.table()<br/>def function-name():<br/><---->return (query)|CREATE OR REFRESH [STREAMING] **```LIVE```** TABLE table_name AS select_statement. **```LIVE```** keyword is used in place of the schema name to refer to the target schema configured for the current DLT pipeline| In SQL, the core logic of transformations is contained in the SELECT statement. In Python, data transformations are specified in the ```return``` clause.  |\n",
    "| @dlt.table(comment = \"Python comment\",table_properties = {\"quality\": \"silver\"}) | COMMENT \"SQL comment\"       TBLPROPERTIES (\"quality\" = \"silver\") | This is how you add comments and table properties in Python vs. SQL |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live Tables vs Streaming Live Tables\n",
    "**Live Tables**\n",
    "* Always \"correct\", meaning their contents will match their definition after any update.\n",
    "* Return same results as if table had just been defined for first time on all data.\n",
    "* Should not be modified by operations external to the DLT Pipeline (you'll either get undefined answers or your change will just be undone).\n",
    "\n",
    "**Streaming Live Tables**\n",
    "* Only supports reading from \"append-only\" streaming sources.\n",
    "* Only reads each input batch once, no matter what (even if joined dimensions change, or if the query definition changes, etc).\n",
    "* Can perform operations on the table outside the managed DLT Pipeline (append data, perform GDPR, etc)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline with DLT\n",
    "Here's an example for using DLT + Auto Loader for Bronze -> Silver -> Gold incremental processing. Some notes about DLT pipeline\n",
    "\n",
    "**Temporary tables**: They can be created with these methods\n",
    "- Declare temporary tables e.g. ```CREATE TEMPORARY LIVE TABLE temp_table```\n",
    "- Create Views e.g. ```CREATE LIVE VIEW temp_table``` Unlike views used elsewhere in Databricks, DLT views are not persisted to the metastore, meaning that they can only be referenced from within the DLT pipeline they are a part of. (This is similar scoping to temporary views in most SQL systems.)\n",
    "\n",
    "\n",
    "**Joins and Referencing Tables Across Notebook Libraries**\n",
    "- Within a DLT Pipeline, code in any notebook library can reference tables and views created in any other notebook library. Essentially, we can think of the scope of the schema reference by the **`LIVE`** keyword to be at the DLT Pipeline level, rather than the individual notebook.\n",
    "\n",
    "**CDC and SCD**\n",
    "- Although not shown in the below code, SCD 1/2 can be applied with ```APPLY CHANGE INTO``` syntax, so that changes from the upstream tables can be propagated into downstream tables. See [here](https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-cdc.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bronze Zone\n",
    "Incremental processing via <a herf=\"https://docs.databricks.com/spark/latest/structured-streaming/auto-loader.html\" target=\"_blank\">Auto Loader</a> (which uses the same processing model as Structured Streaming), requires the addition of the **`STREAMING`** keyword in the declaration as seen below. The **`cloud_files()`** method enables Auto Loader to be used natively with SQL. This method takes the following positional parameters:\n",
    "* The source location, as mentioned above\n",
    "* The source data format, which is JSON in this case\n",
    "* An arbitrarily sized array of optional reader options. In this case, we set **`cloudFiles.inferColumnTypes`** to **`true`**\n",
    "\n",
    "Assume the parameter ```source``` is defined in DLT Pipeline ```Configuration```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING LIVE TABLE sales_orders_raw\n",
    "COMMENT \"The raw sales orders, ingested from retail-org/sales_orders.\"\n",
    "AS SELECT * FROM cloud_files(\"${source}/orders\", \"json\", map(\"cloudFiles.inferColumnTypes\", \"true\"));\n",
    "\n",
    "CREATE OR REFRESH STREAMING LIVE TABLE customers\n",
    "COMMENT \"The customers buying finished products, ingested from retail-org/customers.\"\n",
    "AS SELECT * FROM cloud_files(\"${source}/customers\", \"csv\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silver Zone\n",
    "The **`CONSTRAINT`** keyword introduces quality control. Similar in function to a traditional **`WHERE`** clause, **`CONSTRAINT`** integrates with DLT, enabling it to collect metrics on constraint violations. Constraints provide an optional **`ON VIOLATION`** clause, specifying an action to take on records that violate the constraint. The three modes currently supported by DLT include:\n",
    "\n",
    "| **`ON VIOLATION`** | Behavior |\n",
    "| --- | --- |\n",
    "| **`FAIL UPDATE`** | Pipeline failure when constraint is violated |\n",
    "| **`DROP ROW`** | Discard records that violate constraints |\n",
    "| Omitted | Records violating constraints will be included (but violations will be reported in metrics) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING LIVE TABLE sales_orders_cleaned(\n",
    "  CONSTRAINT valid_order_number EXPECT (order_number IS NOT NULL) ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_id EXPECT (customer_id IS NOT NULL) ON VIOLATION FAIL UPDATE\n",
    ")\n",
    "COMMENT \"The cleaned sales orders with valid order_number(s).\"\n",
    "AS\n",
    "  SELECT f.customer_id, f.customer_name, f.number_of_line_items, \n",
    "         timestamp(from_unixtime((cast(f.order_datetime as long)))) as order_datetime, \n",
    "         c.loyalty_segment\n",
    "  FROM STREAM(LIVE.sales_orders_raw) f\n",
    "  LEFT JOIN LIVE.customers c\n",
    "    ON c.customer_id = f.customer_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold Zone\n",
    "At the most refined level of the architecture, we declare a table delivering an aggregation with business value, in this case a collection of sales order data based in a specific region. In aggregating, the report generates counts and totals of orders by date and customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH LIVE TABLE sales_order_in_la\n",
    "COMMENT \"Sales orders in LA.\"\n",
    "AS\n",
    "  SELECT city, order_date, customer_id, customer_name, ordered_products_explode.curr, \n",
    "         sum(ordered_products_explode.price) as sales, \n",
    "         sum(ordered_products_explode.qty) as quantity, \n",
    "         count(ordered_products_explode.id) as product_count\n",
    "  FROM (SELECT city, order_date, customer_id, customer_name, explode(ordered_products) as ordered_products_explode\n",
    "        FROM LIVE.sales_orders_cleaned \n",
    "        WHERE city = 'Los Angeles')\n",
    "  GROUP BY order_date, city, customer_id, customer_name, ordered_products_explode.curr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Change Data Feed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change data feed allows Databricks to track row-level changes between versions of a Delta table. When enabled on a Delta table, the runtime records change events for all the data written into the table.\n",
    "\n",
    "Here are the codes for enabling this feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This enables CDF for particular tables\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE silverTable SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Or, enable CDF using Spark conf setting in a notebook or on a cluster will ensure it's used on all newly created Delta tables in that scope.\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this is enabled, two things will happen:\n",
    "- A folder ```_change_data``` will appear in the Delta Table directory, which contains parquet files\n",
    "- CDC data of the table can be read using the below methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# via Python\n",
    "cdc_df = (spark.readStream\n",
    "               .format(\"delta\")\n",
    "               .option(\"readChangeData\", True)\n",
    "               .option(\"startingVersion\", 0)\n",
    "               .table(\"silver\"))\n",
    "\n",
    "# via SQL\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM table_changes('silver', 0) order by _commit_timestamp\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are using a table that has updates written to it as a streaming source! This is a **huge** value add, and something that historically has required extensive workarounds to process correctly.\n",
    "\n",
    "This would mean:\n",
    "- **Silver and Gold tables**: Updates to Silver table can be isolated and pushed to Gold table, without reading through the whole Silver table. Data removal requests can also be more easily fulfilled, as DELETE pushed down, see below cell for an example\n",
    "- **Materialized view**s: Create up-to-date, aggregated views of information for use in BI and analytics without having to reprocess the full underlying tables, instead updating only where changes have come through.\n",
    "- **Transmit changes**: Send a change data feed to downstream systems such as Kafka or RDBMS that can use it to incrementally process in later stages of data pipelines.\n",
    "- **Audit trail table**: Capture the change data feed as a Delta table provides perpetual storage and efficient query capability to see all changes over time, including when deletes occur and what updates were made.\n",
    "\n",
    "Please see [here](https://docs.databricks.com/_static/notebooks/delta/cdf-demo.html) for a demo. Below is an example `forEachBatch` function to propagate delete to downstream tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_deletes(microBatchDF, batchId):\n",
    "    \n",
    "    (microBatchDF\n",
    "        .filter(\"_change_type = 'delete'\")\n",
    "        .createOrReplaceTempView(\"deletes\"))\n",
    "    \n",
    "    microBatchDF._jdf.sparkSession().sql(\"\"\"\n",
    "        MERGE INTO users u\n",
    "        USING deletes d\n",
    "        ON u.alt_id = d.alt_id\n",
    "        WHEN MATCHED\n",
    "            THEN DELETE\n",
    "    \"\"\")\n",
    "\n",
    "    microBatchDF._jdf.sparkSession().sql(\"\"\"\n",
    "        DELETE FROM user_bins\n",
    "        WHERE user_id IN (SELECT user_id FROM deletes)\n",
    "    \"\"\")\n",
    "    \n",
    "    microBatchDF._jdf.sparkSession().sql(\"\"\"\n",
    "        MERGE INTO delete_requests dr\n",
    "        USING deletes d\n",
    "        ON d.alt_id = dr.alt_id\n",
    "        WHEN MATCHED\n",
    "          THEN UPDATE SET status = \"deleted\"\n",
    "    \"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other things"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privileges\n",
    "- **SELECT**: gives read access to an object.\n",
    "- **CREATE**: gives ability to create an object (for example, a table in a schema).\n",
    "- **MODIFY**: gives ability to add, delete, and modify data to or from an object.\n",
    "- **USAGE**: does not give any abilities, but is an additional requirement to perform any action on a ```schema``` object.\n",
    "- **READ_METADATA**: gives ability to view an object and its metadata.\n",
    "- **CREATE_NAMED_FUNCTION**: gives ability to create a named UDF in an existing `catalog` or `schema`.\n",
    "- **MODIFY_CLASSPATH**: gives ability to add files to the Spark class path.\n",
    "- **ALL PRIVILEGES**: gives all privileges (is translated into all the above privileges)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster ACL https://docs.databricks.com/security/access-control/cluster-acl.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access modes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Access mode dropdown** | **Access control** | **Visible to user** | **Unity Catalog support** | **Supported languages** |\n",
    "|--------|--------|--------|--------|--------|\n",
    "| Single user | Credential pass-through | Always | Yes | Python, SQL, Scala, R |\n",
    "| Shared | Credential pass-through / Table ACL | Always (**Premium plan required**) | Yes | Python, SQL |\n",
    "| No isolation shared | Maps all the users to the root account, everything is shared, they can install anything, and changes apply to all users | Admins can hide this cluster type by enforcing user isolation in the admin console | No | Python, SQL, Scala, R |\n",
    "| Custom | This option will only be shown for existing clusters without access modes | No | Python, SQL, Scala, R |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below describes how to install custom packages in Compute\n",
    "- **Install in UI**, can be scoped by Workspace (which can be created and installed on all clusters in the Workspace), Cluster (self-explanatory), or Notebook scope (run %pip install xxx in Notebook)\n",
    "- **Init script**, can be defined per cluster / set global init script (not recommended as per Databricks best practice)\n",
    "\n",
    "Pool are reserved idle instances to reduce cluster start-up time. They don't charge DBU, but Cloud Platform instance charges apply however"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Job permissions\n",
    "\n",
    "- A job cannot have a group as an owner\n",
    "- Jobs triggered through Run Now assume the permissions of the **job owner** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Ability** | **No Permissions** | **Can View** | **Can Manage Run** | **Is Owner** | **Can Manage** |\n",
    "|--------|--------|--------|--------|--------|--------|\n",
    "| View job details and settings | x | x | x | x | x|\n",
    "| View results, Spark UI, logs of a job run | | x | x | x | x|\n",
    "| Run now | | | x | x | x|\n",
    "| Cancel run | | | x | x | x|\n",
    "| Edit job settings | | | | x | x|\n",
    "| Modify permissions | | | | x | x|\n",
    "| Delete job | | | | x | x|\n",
    "| Change owner | | | | | |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "- The use of **shared job cluster** is recommended because:\n",
    "  - The cluster can be reused for multiple jobs and reducing the start-up time\n",
    "  - All-purpose compute does not auto-terminate when job finishes\n",
    "- The workflow structure of A -> [B, C] , where A set Spark Configuration for storage credentials for subsequent consumption by B and C, does not work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unity Catalog\n",
    "\n",
    "Data Sharing\n",
    "- Metastore is a regional resource, sharing metastore across regions is done via Delta Share\n",
    "- It might be tempting to create external tables in metastore B to query tables managed in metastore A, but this is strongly advised against because upstream changes in tables recognized in metastore A will not be propagated across\n",
    "- Cross-region/cloud queries could incur additional egress charge. For frequently queried tables, making a direct copy is preferred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Commit Messages\n",
    "\n",
    "Delta Lake supports arbitrary commit messages that will be recorded to the Delta transaction log and viewable in the table history. This can help with later auditing.\n",
    "\n",
    "Setting this with SQL will create a global commit message that will be used for all subsequent operations in our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SET spark.databricks.delta.commitInfo.userMetadata=Deletes committed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With DataFrames, commit messages can also be specified as part of the write options using the **`userMetadata`** option.\n",
    "\n",
    "Here, we'll indicate that we're manually processing these requests in a notebook, rather than using an automated job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "query = (requests_df.writeStream\n",
    "                    .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/delete_requests\")\n",
    "                    .option(\"userMetadata\", \"Requests processed interactively\")\n",
    "                    .trigger(availableNow=True)\n",
    "                    .table(\"delete_requests\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce22748e8bebf5a2be273a894549765c8c30b36b7393e39d528e5cead3b97804"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
