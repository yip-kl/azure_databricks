{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions  import from_unixtime\n",
    "from pyspark.sql.functions  import to_date\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import to_json, struct\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mount storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Databricks use blob storage (ADLS Gen-2) as data source, here's an example of how to mount ADLS Gen-2 Storage FileSystem to DBFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a ADLS-Gen2 storage account, and define mount point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageAccount=\"xxx\" # Name of the ADLS Gen2 Storage Account\n",
    "mountpoint = \"/mnt/xxx\" # Mount the storage account to a chosen path in DBFS\n",
    "storageEndPoint =\"abfss://container_name@{}.dfs.core.windows.net/\".format(storageAccount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, authenticate into the storage endpoint. To do so, these needs to be performed in Azure:\n",
    "1. **Application registration**: You will need to register an Azure Active Directory (AAD) application. On the Azure portal home page, search for \"Azure Active Directory\" &rarr; select App registrations &rarr; New registration.\n",
    "2. **Create secret to the application**: Click on \"Certificates & secrets\" under the Manage heading &rarr; add a new client secret &rarr; Copy the value\n",
    "3. **Grant ADLS-Gen2 access to the registered Application**: In the ADLS-Gen2 storage account, navigate to Access Control (IAM) &rarr; Add &rarr; Add role assignment &rarr; Role = Storage Blob Data Contributor; Assign access to = User, group, or service principal; Select = The registered Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientID =\"xxx\" # Obtained from (1) the registered Application -> Application (client) ID\n",
    "tenantID =\"xxx\" # Obtained from (1) the registered Application -> Directory (tenant) ID\n",
    "clientSecret =\"xxx\" # Obtained from (2) the registered Application -> Copied secret value\n",
    "oauth2Endpoint = \"https://login.microsoftonline.com/{}/oauth2/token\".format(tenantID)\n",
    "\n",
    "\n",
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "           \"fs.azure.account.oauth2.client.id\": clientID,\n",
    "           \"fs.azure.account.oauth2.client.secret\": clientSecret,\n",
    "           \"fs.azure.account.oauth2.client.endpoint\": oauth2Endpoint}\n",
    "\n",
    "try:\n",
    "  dbutils.fs.mount(\n",
    "  source = storageEndPoint,\n",
    "  mount_point = mountpoint,\n",
    "  extra_configs = configs)\n",
    "except Exception as e:\n",
    "  if 'Directory already mounted' in str(e):\n",
    "    print('Directory already mounted')\n",
    "  else:\n",
    "    print(str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some useful commands to inspect the mounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all mount points\n",
    "display(dbutils.fs.mounts())\n",
    "\n",
    "# List files under a specific mount point\n",
    "display(dbutils.fs.ls(mountpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files in the ADLS-Gen2 can thus be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Orders.csv file in a Spark dataframe\n",
    "df_ord= spark.read.format(\"csv\").option(\"header\",True).load(\"dbfs:/mnt/Gen2/csvFiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing the below will create an External table in Databricks for you to read the CSV.\n",
    "# This is not about creating Delta tables however\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE Orders\n",
    "    {{(schema)}}\n",
    "USING {{csv}}\n",
    "OPTIONS (\n",
    "    path 'dbfs:/mnt/Gen2/csvFiles',\n",
    "    header 'true',\n",
    "    delimiter ','\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and writing to Delta Tables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta Table is actually a bunch of snappy-compressed parquet files, with Delta Log files. It offer the following benefits:\n",
    "- Easy rollback since it tracks every changes to the table in the delta log\n",
    "- ACID compliance\n",
    "- Enforce constraint defined in DDL\n",
    "- Optimized performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount path for the raw data blobs\n",
    "original_path = \"/mnt/Gen2/Orders/someFiles\"\n",
    "\n",
    "# Delta output path\n",
    "delta_path = \"/mnt/Gen2/Orders/delta\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example for creating Delta Table using CSV files, these will happen after executing the below:\n",
    "- Under the defined LOCATION, snappy-compressed parquet files will be created, alongside the _delta_log folder\n",
    "- You will find the relevant table under the \"Data\" section in Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Temp View is necessary to allow Databricks to parse the CSV first, before making it a Delta table\n",
    "# LOCATION flag is used to make the table an unmanaged one, whose data does not reside in Databricks but in the specified path\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW Orders_vw\n",
    "    {{(schema)}}\n",
    "USING {{file format e.g. csv}}\n",
    "OPTIONS (\n",
    "    path '{original_path}',\n",
    "    header 'true',\n",
    "    delimiter '|'\n",
    "    );\n",
    "\n",
    "CREATE OR REPLACE TABLE Orders\n",
    "LOCATION '{delta_path}'\n",
    "PARITION BY {{column name}}\n",
    "AS\n",
    "SELECT *, EXTRACT(YEAR FROM Event_Date) FROM Orders_vw\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For self-describing file formats like parquet, the syntax can be made easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This will not work for CSV, beecause the CTAS statement cannot infer schema correctly\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE Orders\n",
    "LOCATION '{delta_path}'\n",
    "PARITION BY {{column name}}\n",
    "AS\n",
    "    SELECT *\n",
    "    FROM parquet.`{original_path}`\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative code using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input into dataframe\n",
    "df_ord = (spark.read.format(\"parquet\").load(original_path)\n",
    "      .withColumn(\"timestamp\", current_timestamp())\n",
    "      .withColumn(\"O_OrderDateYear\", year(col(\"O_OrderDate\")))\n",
    "     )\n",
    "\n",
    "# Save into delta path. Go over to the ADLS Gen2 container and you should see new files got created in the delta path\n",
    "# Files are organized into different folders according to the \"partitionBy\" value\n",
    "df_ord.write.format(\"delta\").partitionBy(\"O_OrderDateYear\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# Execute this then visit the \"Data\" section in Databricks, you will see the relevant table.\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE Orders\n",
    "LOCATION '{delta_path}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creation, the table can be queried in SQL or Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Via SQL\n",
    "%sql\n",
    "SELECT o.*\n",
    "FROM Orders o\n",
    "\n",
    "# Via Python\n",
    "deltaTable = spark.read.format(\"delta\").load(delta_path)\n",
    "deltaTable.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "# View history of the Delta table\n",
    "DESCRIBE HISTORY Orders\n",
    "\n",
    "# Query a specific version\n",
    "SELECT * FROM Orders VERSION AS OF 1\n",
    "\n",
    "# Restore a previous version\n",
    "RESTORE TABLE Orders VERSION AS OF 5\n",
    "\n",
    "# View details of the Delta table e.g. number of files, partitioning, etc.\n",
    "DESCRIBE DETAIL Orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization\n",
    "- Compacting small files and indexing via [OPTIMIZE and ZORDER](https://www.confessionsofadataguy.com/exploring-delta-lakes-zorder-and-performance-on-databricks/)\n",
    "- Remove unused files from a table directory via [VACCUM](https://learn.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/language-manual/delta-vacuum/), add DRY RUN to preview previous versions to be deleted first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tables vs Views vs CTE\n",
    "- Table\n",
    "    - Managed table: Data is actually stored in DBFS\n",
    "    - Unmanaged table: Data is stored in elsewhere e.g. ADLS-Gen2\n",
    "- Views\n",
    "    - View: Will persist like table\n",
    "    - Temp View: Persist in the current notebook session only\n",
    "    - Global Temp View: Can be shared across different notebook sessions, until cluster restarts\n",
    "- CTE\n",
    "    - CTE: Referenced within the scope of a SQL statement only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming data pipeline from EventHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of building the Bronze, Silver, and Gold Zone for a streaming data pipeline\n",
    "- Bronze: Read live data from EventHub, and historical data from ADLS-Gen2. Parse content and union them\n",
    "- Silver: Implement business rules and data cleansing process and join with lookup tables\n",
    "- Gold: Data is aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "db_name = \"VehicleSensor\"\n",
    "\n",
    "def get_config(zone):\n",
    "    return {\n",
    "    'delta_path': f\"/mnt/SensorData/vehiclestreamingdata/{zone}/delta\",\n",
    "    'chkpt_path': f\"/mnt/SensorData/vehiclestreamingdata/{zone}/chkpt\",\n",
    "    'delta_table': f\"VehicleDelta_{zone}\"\n",
    "    }\n",
    "\n",
    "# Create DB first\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS{db_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronze Zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming from Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spark DataFrame which reads from the Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.4 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Yip/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "TOPIC = \"cookbook-eventhub\" # Event Hub namespace\n",
    "BOOTSTRAP_SERVERS = \"cookbook-eventhub.servicebus.windows.net:9093\" # Host name of Event Hub:9093, 9093 is the port for Kafka\n",
    "\n",
    "# Go to Event Hub's Shared Access Policies -> Click onto a policy -> Copy Connection stringâ€“primary key here\n",
    "CONN_STRING = \"Endpoint=sb://kafkaenabledeventhubns.servicebus.windows.net/;SharedAccessKeyName=sendreceivekafka;SharedAccessKey=4vxbVwasdasdsdasd4aVcUWBvYp44sdasaasasasasasasvoVE=\" \n",
    "\n",
    "# The $ConnectionString and $Default are fixed values, don't update them\n",
    "EH_SASL = EH_SASL = f\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"$ConnectionString\\\" password=\\\"{CONN_STRING}\\\";\"\n",
    "GROUP_ID = \"$Default\" \n",
    "\n",
    "# // Read stream using Spark SQL (structured streaming)\n",
    "# // consider adding .option(\"startingOffsets\", \"earliest\") to read earliest available offset during testing\n",
    "kafkaDF = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"subscribe\", TOPIC) \\\n",
    "    .option(\"kafka.bootstrap.servers\", BOOTSTRAP_SERVERS) \\\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n",
    "    .option(\"kafka.sasl.jaas.config\", EH_SASL) \\\n",
    "    .option(\"kafka.request.timeout.ms\", \"60000\") \\\n",
    "    .option(\"kafka.session.timeout.ms\", \"60000\") \\\n",
    "    .option(\"kafka.group.id\", \"POC\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load() \\\n",
    "    .withColumn(\"source\", lit(TOPIC)) # Optional: Also add the topic as column\n",
    "\n",
    "#Check if streaming is on and getting the schema for the kakfa dataframe \n",
    "print(kafkaDF.isStreaming)\n",
    "print(kafkaDF.printSchema())\n",
    "\n",
    "#It should then output something like this:\n",
    "#\n",
    "#True\n",
    "#root\n",
    "# |-- key: binary (nullable = true)\n",
    "# |-- value: binary (nullable = true)\n",
    "# |-- topic: string (nullable = true)\n",
    "# |-- partition: integer (nullable = true)\n",
    "# |-- offset: long (nullable = true)\n",
    "# |-- timestamp: timestamp (nullable = true)\n",
    "# |-- timestampType: integer (nullable = true)\n",
    "# |-- source: string (nullable = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the Kafka message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the schema for the 'value' field in Kafka message\n",
    "jsonschema = StructType() \\\n",
    ".add(\"id\", StringType()) \\\n",
    ".add(\"eventtime\", TimestampType()) \\\n",
    ".add(\"rpm\", IntegerType()) \\\n",
    ".add(\"speed\", IntegerType()) \\\n",
    ".add(\"kms\", IntegerType()) \\\n",
    ".add(\"lfi\", IntegerType())  \\\n",
    ".add(\"lat\", DoubleType()) \\\n",
    ".add(\"long\", DoubleType())\n",
    "\n",
    "# Select the key and value\n",
    "newkafkaDF=kafkaDF.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"source\") \\\n",
    "    .withColumn('vehiclejson', from_json(col('value'),schema=jsonschema))\n",
    "\n",
    "# Flatten the json\n",
    "kafkajsonDF=newkafkaDF.select(\"key\",\"value\", \"vehiclejson.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the streaming data to Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Delta files. Checkpoint is set so that it can recover from failure in the event of server failure\n",
    "query=kafkajsonDF.selectExpr(\n",
    "                  \"id\"\t  \\\n",
    "                  ,\"eventtime\"\t   \\\n",
    "                  ,\"rpm\"\t\\\n",
    "                  ,\"speed\" \\\n",
    "                  ,\"kms\" \\\n",
    "                  ,\"lfi\" \\\n",
    "                  ,\"lat\" \\\n",
    "                  ,\"long\" \\\n",
    "                  ,\"source\") \\\n",
    "            .writeStream.format(\"delta\") \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .option(\"checkpointLocation\",get_config(\"Bronze\")['chkpt_path']) \\\n",
    "            .start(get_config(\"Bronze\")['delta_path']) \n",
    "\n",
    "# Data can thus be streamed to a Delta table \n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {db_name}.{get_config(\"Bronze\")['delta_table']}\n",
    "USING DELTA LOCATION '{get_config(\"Bronze\")['delta_path']}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming from ADLS-Gen2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you have historical data in ADLS-Gen2, and you would like to stream-read the data there as files are ingested there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from ADLS gen-2 using readStream API and writing as delta format\n",
    "def append_batch_source():\n",
    "  \n",
    "  topic =\"historical\"\n",
    "  mnt_path = \"xxx\"\n",
    "\n",
    "  kafkaDF = (spark.readStream \\\n",
    "    .schema(jsonschema)\n",
    "    .format(\"parquet\") \\\n",
    "    .load(mnt_path).withColumn(\"source\", lit(topic)))\n",
    "\n",
    "  # See here about the explanations of different options for trigger\n",
    "  # https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers\n",
    "  query=kafkaDF.selectExpr(\n",
    "                  \"id\"\t  \\\n",
    "                  ,\"eventtime\"\t   \\\n",
    "                  ,\"rpm\"\t\\\n",
    "                  ,\"speed\" \\\n",
    "                  ,\"kms\" \\\n",
    "                  ,\"lfi\" \\\n",
    "                  ,\"lat\" \\\n",
    "                  ,\"long\" \\\n",
    "                  ,\"source\"\n",
    "                  ) \\\n",
    "            .writeStream.format(\"delta\") \\\n",
    "            .option(\"checkpointLocation\",get_config(\"Hist\")['chkpt_path']) \\\n",
    "            .trigger(processingTime='2 seconds') \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .start(get_config(\"Hist\")['delta_path']) \n",
    "\n",
    "  return query\n",
    "\n",
    "\n",
    "# Create historical delta table\n",
    "append_batch_source()\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {db_name}.{get_config(\"Historical\")['delta_table']}\n",
    "USING DELTA LOCATION '{get_config(\"Historical\")['delta_path']}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect schema\n",
    "spark.sql(f\"\"\"\n",
    "describe formatted {db_name}.{get_config(\"Bronze\")['delta_table']}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union the live and historical data, and generate Temp View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With live and historical delta tables created, we can union them for subsequent usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Streaming Data from Bronze and Historical tables\n",
    "df_bronze=spark.readStream.format(\"delta\").option(\"latestFirst\", \"true\").table(f\"{db_name}.{get_config(\"Bronze\")['delta_table']}\")\n",
    "df_historical=spark.readStream.format(\"delta\").option(\"latestFirst\", \"true\").table(f\"{db_name}.{get_config(\"Historical\")['delta_table']}\")\n",
    "\n",
    "#Joining both historical and Bronze Streaming Data. The TempView can be used like CTE in SQL statements\n",
    "df_bronze_all = df_bronze.union(df_historical)\n",
    "df_bronze_all.createOrReplaceTempView(\"vw_TempBronzeAll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(*) from vw_TempBronzeAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver Zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Azure SQL DB for lookup tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish connection to Azure SQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config details for Azure SQL DB for VehicleInformation and LocationInformation tables\n",
    "sqldbusername = dbutils.secrets.get(scope=\"KeyVaultScope\",key=\"VehicleInformationDBUserId\")\n",
    "sqldbpwd = dbutils.secrets.get(scope=\"KeyVaultScope\",key=\"VehicleInformationDBPwd\")\n",
    "\n",
    "jdbcHostname = \"vehicledemoinformatiosrvr.database.windows.net\"\n",
    "jdbcDatabase = \"VehicleInformationDB\"\n",
    "jdbcPort = 1433\n",
    "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2};user={3};password={4}\".format(jdbcHostname, jdbcPort, jdbcDatabase, sqldbusername, sqldbpwd)\n",
    "connectionProperties = {\n",
    "  \"user\" : sqldbusername,\n",
    "  \"password\" : sqldbpwd,\n",
    "  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the two lookup tables and create Temp Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading dbo.VehicleInfo master table from Azure SQL DB and creating a view\n",
    "vehicleInfo = \"(select VehicleId,Make,Model,Category,ModelYear from dbo.VehicleInformation) vehicle\"\n",
    "df_vehicleInfo = spark.read.jdbc(url=jdbcUrl, table=vehicleInfo, properties=connectionProperties)\n",
    "df_vehicleInfo.createOrReplaceTempView(\"vw_VehicleMaster\")\n",
    "display(df_vehicleInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading dbo.LocationInfo master table from Azure SQL DB and creating a view\n",
    "locationInfo = \"(select Borough,Location,Latitude,Longitude from dbo.LocationInfo) vehicle\"\n",
    "df_locationInfo = spark.read.jdbc(url=jdbcUrl, table=locationInfo, properties=connectionProperties)\n",
    "df_locationInfo.createOrReplaceTempView(\"vw_LocationMaster\")\n",
    "display(df_locationInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join data and output Silver table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the Bronze data with Lookup Tables, and save data in Delta format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputMode(\"append\"): output only new rows to the output sink\n",
    "# option(\"MergeSchema\",\"True\"): merging the schema between the Bronze and lookup tables\n",
    "\n",
    "df_silver= spark.sql(\"select s.*,m.Make,m.Model,m.Category, Year(eventtime) as Year, month(eventtime) as Month,day(eventtime) as Day, \\\n",
    "                     hour(eventtime) as Hour,l.Borough,l.Location  \\\n",
    "                     from vw_TempBronzeAll s \\\n",
    "                     left join vw_VehicleMaster m on s.id = m.VehicleId \\\n",
    "                     left join vw_LocationMaster l on s.lat = l.Latitude and s.long = l.Longitude\") \\\n",
    "            .writeStream.format(\"delta\").option(\"MergeSchema\",\"True\") \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .option(\"checkpointLocation\",get_config(\"Silver\")['chkpt_path'])  \\\n",
    "            .start(get_config(\"Silver\")['delta_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Delta table for Silver Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Delta table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {db_name}.{get_config(\"Silver\")['delta_table']}\n",
    "USING DELTA LOCATION '{get_config(\"Silver\")['delta_path']}'\n",
    "\"\"\")\n",
    "\n",
    "# Inspect schema\n",
    "spark.sql(f\"\"\"\n",
    "describe formatted {db_name}.{get_config(\"Silver\")['delta_table']}\n",
    "\"\"\")\n",
    "\n",
    "# Run the following code to read data as streaming data from the Delta table.\n",
    "display(spark.readStream.format(\"delta\").table(get_config(\"Silver\")['delta_table']).groupBy(\"Make\").count().orderBy(\"Make\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the data and write to Gold Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data in Delta format\n",
    "df_gold=(\n",
    "spark.readStream.format(\"delta\").option(\"latestFirst\", \"true\").table(f\"{db_name}.{get_config(\"Silver\")['delta_table']}\")\n",
    "    # Apply Watermark to handle late data and perform aggeregation. See here for a discussion about withWatermark and outputMode\n",
    "    # https://dvirgiln.github.io/spark-structured-streaming-output-modes/\n",
    "    .withWatermark(\"timestamp\",\"4 minutes\")\n",
    "    .groupBy(window('eventtime',\"1 hour\"),\"Make\",\"Borough\",\"Location\",\"Month\",\"Day\",\"Hour\").count()) \\\n",
    "        .writeStream.format(\"delta\") \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .option(\"checkpointLocation\", get_config(\"Gold\")['chkpt_path']) \\\n",
    "        .start(get_config(\"Gold\")['delta_path'])\n",
    "\n",
    "# Create Delta table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {db_name}.{get_config(\"Gold\")['delta_table']}\n",
    "USING DELTA LOCATION '{get_config(\"Gold\")['delta_path']}'\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce22748e8bebf5a2be273a894549765c8c30b36b7393e39d528e5cead3b97804"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
