{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions  import from_unixtime\n",
    "from pyspark.sql.functions  import to_date\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import to_json, struct\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mount storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Databricks use blob storage (ADLS Gen-2) as data source, here's an example of how to mount ADLS Gen-2 Storage FileSystem to DBFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a ADLS-Gen2 storage account, and define mount point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageAccount=\"xxx\" # Name of the ADLS Gen2 Storage Account\n",
    "mountpoint = \"/mnt/xxx\" # Mount the storage account to a chosen path in DBFS\n",
    "storageEndPoint =\"abfss://container_name@{}.dfs.core.windows.net/\".format(storageAccount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, authenticate into the storage endpoint. To do so, these needs to be performed in Azure:\n",
    "1. **Application registration**: You will need to register an Azure Active Directory (AAD) application. On the Azure portal home page, search for \"Azure Active Directory\" &rarr; select App registrations &rarr; New registration.\n",
    "2. **Create secret to the application**: Click on \"Certificates & secrets\" under the Manage heading &rarr; add a new client secret &rarr; Copy the value\n",
    "3. **Grant ADLS-Gen2 access to the registered Application**: In the ADLS-Gen2 storage account, navigate to Access Control (IAM) &rarr; Add &rarr; Add role assignment &rarr; Role = Storage Blob Data Contributor; Assign access to = User, group, or service principal; Select = The registered Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientID =\"xxx\" # Obtained from (1) the registered Application -> Application (client) ID\n",
    "tenantID =\"xxx\" # Obtained from (1) the registered Application -> Directory (tenant) ID\n",
    "clientSecret =\"xxx\" # Obtained from (2) the registered Application -> Copied secret value\n",
    "oauth2Endpoint = \"https://login.microsoftonline.com/{}/oauth2/token\".format(tenantID)\n",
    "\n",
    "\n",
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "           \"fs.azure.account.oauth2.client.id\": clientID,\n",
    "           \"fs.azure.account.oauth2.client.secret\": clientSecret,\n",
    "           \"fs.azure.account.oauth2.client.endpoint\": oauth2Endpoint}\n",
    "\n",
    "try:\n",
    "  dbutils.fs.mount(\n",
    "  source = storageEndPoint,\n",
    "  mount_point = mountpoint,\n",
    "  extra_configs = configs)\n",
    "except Exception as e:\n",
    "  if 'Directory already mounted' in str(e):\n",
    "    print('Directory already mounted')\n",
    "  else:\n",
    "    print(str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some useful commands to inspect the mounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all mount points\n",
    "display(dbutils.fs.mounts())\n",
    "\n",
    "# List files under a specific mount point\n",
    "display(dbutils.fs.ls(mountpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files in the ADLS-Gen2 can thus be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Orders.csv file in a Spark dataframe\n",
    "df_ord= spark.read.format(\"csv\").option(\"header\",True).load(\"dbfs:/mnt/Gen2/csvFiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing the below will create an External table in Databricks for you to read the CSV.\n",
    "# This is not about creating Delta tables however\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE Orders\n",
    "    {{(schema)}}\n",
    "USING {{csv}}\n",
    "OPTIONS (\n",
    "    path 'dbfs:/mnt/Gen2/csvFiles',\n",
    "    header 'true',\n",
    "    delimiter ','\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and writing to Delta Tables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta Table is actually a bunch of snappy-compressed parquet files, with Delta Log files. It offer the following benefits:\n",
    "- Easy rollback since it tracks every changes to the table in the delta log\n",
    "- ACID compliance\n",
    "- Enforce constraint defined in DDL\n",
    "- Optimized performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount path for the raw data blobs\n",
    "original_path = \"/mnt/Gen2/Orders/someFiles\"\n",
    "\n",
    "# Delta output path\n",
    "delta_path = \"/mnt/Gen2/Orders/delta\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example for creating Delta Table using CSV files, these will happen after executing the below:\n",
    "- Under the defined LOCATION, snappy-compressed parquet files will be created, alongside the _delta_log folder\n",
    "- You will find the relevant table under the \"Data\" section in Databricks\n",
    "\n",
    "The table information will persist in Databricks's metastore even when you shut down the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Temp View is necessary to allow Databricks to parse the CSV first, before making it a Delta table\n",
    "# LOCATION flag is used to make the table an unmanaged one, whose data does not reside in Databricks but in the specified path\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW Orders_vw\n",
    "    {{(schema, if necessary)}}\n",
    "USING {{file format e.g. csv}}\n",
    "OPTIONS (\n",
    "    path '{original_path}',\n",
    "    header 'true',\n",
    "    delimiter '|'\n",
    "    );\n",
    "\n",
    "CREATE OR REPLACE TABLE Orders\n",
    "LOCATION '{delta_path}'\n",
    "PARITION BY {{column name}}\n",
    "AS\n",
    "SELECT *, EXTRACT(YEAR FROM Event_Date) FROM Orders_vw\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For self-describing file formats like parquet, the syntax can be made easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This will not work for CSV, beecause the CTAS statement cannot infer schema correctly\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE Orders\n",
    "LOCATION '{delta_path}'\n",
    "PARITION BY {{column name}}\n",
    "AS\n",
    "    SELECT *\n",
    "    FROM parquet.`{original_path}`\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative code using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input into dataframe\n",
    "df_ord = (spark.read.format(\"parquet\").load(original_path)\n",
    "      .withColumn(\"timestamp\", current_timestamp())\n",
    "      .withColumn(\"O_OrderDateYear\", year(col(\"O_OrderDate\")))\n",
    "     )\n",
    "\n",
    "# Save into delta path. Go over to the ADLS Gen2 container and you should see new files got created in the delta path\n",
    "# Files are organized into different folders according to the \"partitionBy\" value\n",
    "df_ord.write.format(\"delta\").partitionBy(\"O_OrderDateYear\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# Execute this then visit the \"Data\" section in Databricks, you will see the relevant table.\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE Orders\n",
    "LOCATION '{delta_path}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creation, the table can be queried in SQL or Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Via SQL\n",
    "%sql\n",
    "SELECT o.*\n",
    "FROM Orders o\n",
    "\n",
    "# Via Python\n",
    "deltaTable = spark.read.format(\"delta\").load(delta_path)\n",
    "deltaTable.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "# View history of the Delta table\n",
    "DESCRIBE HISTORY Orders\n",
    "\n",
    "# Query a specific version\n",
    "SELECT * FROM Orders VERSION AS OF 1\n",
    "\n",
    "# Restore a previous version\n",
    "RESTORE TABLE Orders VERSION AS OF 5\n",
    "\n",
    "# View details of the Delta table e.g. number of files, partitioning, etc.\n",
    "DESCRIBE DETAIL Orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tables vs Views vs CTE\n",
    "- Table\n",
    "    - Managed table: Data is actually stored in DBFS\n",
    "    - Unmanaged table: Data is stored in elsewhere e.g. ADLS-Gen2\n",
    "- Views\n",
    "    - View: Will persist like table\n",
    "    - Temp View: Persist in the current notebook session only\n",
    "    - Global Temp View: Can be shared across different notebook sessions, until cluster restarts\n",
    "- CTE\n",
    "    - CTE: Referenced within the scope of a SQL statement only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming data pipeline from EventHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of building the Bronze, Silver, and Gold Zone for a streaming data pipeline\n",
    "- Bronze: Read live data from EventHub, and historical data from ADLS-Gen2. Parse content and union them\n",
    "- Silver: Implement business rules and data cleansing process and join with lookup tables\n",
    "- Gold: Data is aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "db_name = \"VehicleSensor\"\n",
    "\n",
    "def get_config(zone):\n",
    "    return {\n",
    "    'delta_path': f\"/mnt/SensorData/vehiclestreamingdata/{zone}/delta\",\n",
    "    'chkpt_path': f\"/mnt/SensorData/vehiclestreamingdata/{zone}/chkpt\",\n",
    "    'delta_table': f\"VehicleDelta_{zone}\"\n",
    "    }\n",
    "\n",
    "# Create DB first\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS{db_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronze Zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming from Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spark DataFrame which reads from the Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC = \"cookbook-eventhub\" # Event Hub namespace\n",
    "BOOTSTRAP_SERVERS = \"cookbook-eventhub.servicebus.windows.net:9093\" # Host name of Event Hub:9093, 9093 is the port for Kafka\n",
    "\n",
    "# Go to Event Hub's Shared Access Policies -> Click onto a policy -> Copy Connection stringâ€“primary key here\n",
    "CONN_STRING = \"Endpoint=sb://kafkaenabledeventhubns.servicebus.windows.net/;SharedAccessKeyName=sendreceivekafka;SharedAccessKey=4vxbVwasdasdsdasd4aVcUWBvYp44sdasaasasasasasasvoVE=\" \n",
    "\n",
    "# The $ConnectionString and $Default are fixed values, don't update them\n",
    "EH_SASL = EH_SASL = f\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"$ConnectionString\\\" password=\\\"{CONN_STRING}\\\";\"\n",
    "GROUP_ID = \"$Default\" \n",
    "\n",
    "# // Read stream using Spark SQL (structured streaming)\n",
    "# // consider adding .option(\"startingOffsets\", \"earliest\") to read earliest available offset during testing\n",
    "kafkaDF = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"subscribe\", TOPIC) \\\n",
    "    .option(\"kafka.bootstrap.servers\", BOOTSTRAP_SERVERS) \\\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n",
    "    .option(\"kafka.sasl.jaas.config\", EH_SASL) \\\n",
    "    .option(\"kafka.request.timeout.ms\", \"60000\") \\\n",
    "    .option(\"kafka.session.timeout.ms\", \"60000\") \\\n",
    "    .option(\"kafka.group.id\", \"POC\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load() \\\n",
    "    .withColumn(\"source\", lit(TOPIC)) # Optional: Also add the topic as column\n",
    "\n",
    "#Check if streaming is on and getting the schema for the kakfa dataframe \n",
    "print(kafkaDF.isStreaming)\n",
    "print(kafkaDF.printSchema())\n",
    "\n",
    "#It should then output something like this:\n",
    "#\n",
    "#True\n",
    "#root\n",
    "# |-- key: binary (nullable = true)\n",
    "# |-- value: binary (nullable = true)\n",
    "# |-- topic: string (nullable = true)\n",
    "# |-- partition: integer (nullable = true)\n",
    "# |-- offset: long (nullable = true)\n",
    "# |-- timestamp: timestamp (nullable = true)\n",
    "# |-- timestampType: integer (nullable = true)\n",
    "# |-- source: string (nullable = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the Kafka message and writing the streaming data to Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the schema for the 'value' field in Kafka message\n",
    "jsonschema = StructType() \\\n",
    "      .add(\"id\", StringType()) \\\n",
    "      .add(\"eventtime\", TimestampType()) \\\n",
    "      .add(\"rpm\", IntegerType()) \\\n",
    "      .add(\"speed\", IntegerType()) \\\n",
    "      .add(\"kms\", IntegerType()) \\\n",
    "      .add(\"lfi\", IntegerType())  \\\n",
    "      .add(\"lat\", DoubleType()) \\\n",
    "      .add(\"long\", DoubleType())\n",
    "\n",
    "def parse_to_bronze(kafkaDF, value_schema):\n",
    "\n",
    "      # Parse, and write.\n",
    "      # Checkpoint is set so that it can recover from failure in the event of server failure\n",
    "      # Trigger is for controlling the frequency of writes. For incremental batch jobs. availableNow=True is recommended\n",
    "      parsedDF=kafkaDF.selectExpr(\"CAST(key AS STRING) as key\", \"CAST(value AS STRING) as value\", \"source\") \\\n",
    "            .withColumn('vehiclejson', from_json(col('value'), value_schema)) \\\n",
    "            .select(\"key\", \"value\", \"source\", \"vehiclejson.*\") \\\n",
    "            .writeStream.format(\"delta\") \\\n",
    "            .trigger(processingTime = \"2 minutes\") \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .option(\"checkpointLocation\",get_config(\"Bronze\")['chkpt_path']) \\\n",
    "            .start(get_config(\"Bronze\")['delta_path']) \n",
    "\n",
    "      return parsedDF\n",
    "\n",
    "# Create writeStream\n",
    "parse_to_bronze(kafkaDF, jsonschema)\n",
    "# Data can thus be streamed to a Delta table \n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {db_name}.{get_config(\"Bronze\")['delta_table']}\n",
    "USING DELTA LOCATION '{get_config(\"Bronze\")['delta_path']}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming from ADLS-Gen2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you have historical data in ADLS-Gen2, and you would like to stream-read the data as files are ingested there, AutoLoader is the prefered method.\n",
    "\n",
    "This method is also useful for batch jobs, as likely should be the case here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from ADLS gen-2 using readStream API and writing as delta format\n",
    "def append_batch_source(data_source_mnt_path, value_schema):\n",
    "  \n",
    "  kafkaDF = (spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"parquet\")\n",
    "    .option(\"cloudFiles.schemaLocation\", get_config(\"Hist\")['chkpt_path'])\n",
    "    .load(data_source_mnt_path)\n",
    "  )\n",
    "\n",
    "  parsedDF=kafkaDF.withColumn(\"source\", lit('historical')) \\\n",
    "            .selectExpr(\"CAST(key AS STRING) as key\", \"CAST(value AS STRING) as value\", \"source\") \\\n",
    "            .withColumn('vehiclejson', from_json(col('value'), value_schema)) \\\n",
    "            .select(\"key\", \"value\", \"source\", \"vehiclejson.*\") \\\n",
    "            .writeStream.format(\"delta\") \\\n",
    "            .trigger(availableNow=True) \\\n",
    "            .option(\"checkpointLocation\",get_config(\"Hist\")['chkpt_path']) \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .start(get_config(\"Hist\")['delta_path']) \n",
    "\n",
    "  return parsedDF\n",
    "\n",
    "# Create historical delta table\n",
    "append_batch_source('/xxx', jsonschema)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {db_name}.{get_config(\"Historical\")['delta_table']}\n",
    "USING DELTA LOCATION '{get_config(\"Historical\")['delta_path']}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect schema\n",
    "spark.sql(f\"\"\"\n",
    "DESCRIBE FORMATTED {db_name}.{get_config(\"Bronze\")['delta_table']}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union the live and historical data, and generate Temp View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With live and historical delta tables created, we can union them for subsequent usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Streaming Data from Bronze and Historical tables\n",
    "df_bronze=spark.readStream.format(\"delta\").option(\"latestFirst\", \"true\").table(f\"{db_name}.{get_config(\"Bronze\")['delta_table']}\")\n",
    "df_historical=spark.readStream.format(\"delta\").option(\"latestFirst\", \"true\").table(f\"{db_name}.{get_config(\"Historical\")['delta_table']}\")\n",
    "\n",
    "#Joining both historical and Bronze Streaming Data. The TempView can be used like CTE in SQL statements\n",
    "df_bronze_all = df_bronze.union(df_historical)\n",
    "df_bronze_all.createOrReplaceTempView(\"vw_TempBronzeAll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(*) from vw_TempBronzeAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver Zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Azure SQL DB for lookup tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish connection to Azure SQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config details for Azure SQL DB for VehicleInformation and LocationInformation tables\n",
    "sqldbusername = dbutils.secrets.get(scope=\"KeyVaultScope\",key=\"VehicleInformationDBUserId\")\n",
    "sqldbpwd = dbutils.secrets.get(scope=\"KeyVaultScope\",key=\"VehicleInformationDBPwd\")\n",
    "\n",
    "jdbcHostname = \"vehicledemoinformatiosrvr.database.windows.net\"\n",
    "jdbcDatabase = \"VehicleInformationDB\"\n",
    "jdbcPort = 1433\n",
    "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2};user={3};password={4}\".format(jdbcHostname, jdbcPort, jdbcDatabase, sqldbusername, sqldbpwd)\n",
    "connectionProperties = {\n",
    "  \"user\" : sqldbusername,\n",
    "  \"password\" : sqldbpwd,\n",
    "  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the two lookup tables and create Temp Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading dbo.VehicleInfo master table from Azure SQL DB and creating a view\n",
    "vehicleInfo = \"(select VehicleId,Make,Model,Category,ModelYear from dbo.VehicleInformation) vehicle\"\n",
    "df_vehicleInfo = spark.read.jdbc(url=jdbcUrl, table=vehicleInfo, properties=connectionProperties)\n",
    "df_vehicleInfo.createOrReplaceTempView(\"vw_VehicleMaster\")\n",
    "display(df_vehicleInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading dbo.LocationInfo master table from Azure SQL DB and creating a view\n",
    "locationInfo = \"(select Borough,Location,Latitude,Longitude from dbo.LocationInfo) vehicle\"\n",
    "df_locationInfo = spark.read.jdbc(url=jdbcUrl, table=locationInfo, properties=connectionProperties)\n",
    "df_locationInfo.createOrReplaceTempView(\"vw_LocationMaster\")\n",
    "display(df_locationInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-duplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Kafka provides at-least-once guarantees on data delivery, all Kafka consumers should be prepared to handle duplicate reocrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped_df = (spark.readStream\n",
    "                   .table(\"vw_TempBronzeAll\")\n",
    "                   .select(\"*\")\n",
    "                   .withWatermark(\"time\", \"30 seconds\")\n",
    "                   .dropDuplicates([\"id\", \"eventtime\"])\n",
    "                   .createOrReplaceTempView(\"vw_TempBronzeAll_dedup\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta Lake has optimized functionality for insert-only merges. This operation is ideal for de-duplication: define logic to match on unique keys, and only insert those records for keys that don't already exist.\n",
    "\n",
    "Note that in this application, we proceed in this fashion because we know two records with the same matching keys represent the same information. If the later arriving records indicated a necessary change to an existing record, we would need to change our logic to include a **`WHEN MATCHED`** clause.\n",
    "\n",
    "A merge into query is defined in SQL below against a view titled **`stream_updates`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = f\"\"\"\n",
    "  MERGE INTO {get_config(\"Silver\")['delta_table']} a\n",
    "  USING (SELECT\n",
    "    stream_updates.id\n",
    "    ,stream_updates.eventtime\n",
    "    ,stream_updates.rpm\n",
    "    ,Year(eventtime) as Year\n",
    "    ,month(eventtime) as Month\n",
    "    ,day(eventtime) as Day\n",
    "    ,hour(eventtime) as Hour\n",
    "    ,m.Make\n",
    "    ,m.Model\n",
    "    ,m.Category\n",
    "    ,l.Borough\n",
    "    ,l.Location\n",
    "    FROM stream_updates\n",
    "    LEFT JOIN vw_VehicleMaster m on stream_updates.id = m.VehicleId\n",
    "    LEFT join vw_LocationMaster l on stream_updates.lat = l.Latitude and stream_updates.long = l.Longitude\n",
    "    ) b\n",
    "  ON a.VehicleId=b.VehicleId AND a.eventtime=b.eventtime\n",
    "  WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark Structured Streaming **`foreachBatch`** method allows users to define custom logic when writing.\n",
    "\n",
    "The logic applied during **`foreachBatch`** addresses the present microbatch as if it were a batch (rather than streaming) data, thus enabling the use of some functions, like merge, or window functions. Otherwise it would return error, see the end of this section\n",
    "\n",
    "The class defined in the following cell defines simple logic that will allow us to register any SQL **`MERGE INTO`** query for use in a Structured Streaming write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_to_delta(microBatchDF, batchId):\n",
    "    microBatchDF.createOrReplaceTempView(\"stream_updates\")\n",
    "    microBatchDF._jdf.sparkSession().sql(\"\"\"\n",
    "    MERGE INTO {get_config(\"Silver\")['delta_table']} a\n",
    "    USING (SELECT\n",
    "        stream_updates.id\n",
    "        ,stream_updates.eventtime\n",
    "        ,stream_updates.rpm\n",
    "        ,Year(eventtime) as Year\n",
    "        ,month(eventtime) as Month\n",
    "        ,day(eventtime) as Day\n",
    "        ,hour(eventtime) as Hour\n",
    "        ,m.Make\n",
    "        ,m.Model\n",
    "        ,m.Category\n",
    "        ,l.Borough\n",
    "        ,l.Location\n",
    "        FROM stream_updates\n",
    "        LEFT JOIN vw_VehicleMaster m on stream_updates.id = m.VehicleId\n",
    "        LEFT join vw_LocationMaster l on stream_updates.lat = l.Latitude and stream_updates.long = l.Longitude\n",
    "        ) b\n",
    "    ON a.VehicleId=b.VehicleId AND a.eventtime=b.eventtime\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're using SQL to write to our Delta table, we'll need to make sure this table exists before we begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {get_config(\"Silver\")['delta_table']}\n",
    "(id STRING, eventtime TIMESTAMP, rpm INTEGER, Year INTEGER, Month INTEGER, Day INTEGER, Hour INTEGER, \n",
    "Make STRING, Model STRING, Category STRING, Borough STRING, Location STRING)\n",
    "USING DELTA\n",
    "LOCATION {get_config(\"Silver\")['delta_path']}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the upsert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the previously defined function in our **`foreachBatch`** logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver = (deduped_df.writeStream\n",
    "            .format(\"delta\") \\\n",
    "            .foreachBatch(upsert_to_delta)\n",
    "            .outputMode(\"update\") \\\n",
    "            .option(\"checkpointLocation\",get_config(\"Silver\")['chkpt_path'])  \\\n",
    "            .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to read data as streaming data from the Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.readStream.format(\"delta\").table(get_config(\"Silver\")['delta_table']).groupBy(\"Make\").count().orderBy(\"Make\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If we try to apply this to a streaming read of our data without using ```foreachBatch```, we'll learn that\n",
    "> Non-time-based windows are not supported on streaming DataFrames\n",
    "\n",
    "Below is an example for an attempt to perform a window-function based deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranked_df = (spark.readStream\n",
    "#                   .table(\"bronze\")\n",
    "#                   .filter(\"topic = 'user_info'\")\n",
    "#                   .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "#                   .select(\"v.*\")\n",
    "#                   .filter(F.col(\"update_type\").isin([\"new\", \"update\"]))\n",
    "#                   .withColumn(\"rank\", F.rank().over(window))\n",
    "#                   .filter(\"rank == 1\").drop(\"rank\"))\n",
    "\n",
    "# display(ranked_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality enforcement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be implemented by:\n",
    "1. Enforcing Table Constraints, and sending the unconforming data to a Delta table\n",
    "2. Adding a CASE WHEN flag to the Silver table itself\n",
    "\n",
    "Below is a demonstration of #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table constraints apply boolean filters to columns within a table and prevent data that does not fulfill these constraints from being written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "ALTER TABLE heart_rate_silver ADD CONSTRAINT validbpm CHECK (heartrate > 0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a table to store quarantined records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS bpm_quarantine\n",
    "    (device_id LONG, time TIMESTAMP, heartrate DOUBLE)\n",
    "USING DELTA\n",
    "LOCATION '${da.paths.user_db}/bpm_quarantine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Structured Streaming operations, writing to an additional table can be accomplished within **`foreachBatch`** logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "MERGE INTO heart_rate_silver a\n",
    "USING stream_updates b\n",
    "ON a.device_id=b.device_id AND a.time=b.time\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\"\n",
    "\n",
    "class Upsert:\n",
    "    def __init__(self, query, update_temp=\"stream_updates\"):\n",
    "        self.query = query\n",
    "        self.update_temp = update_temp \n",
    "        \n",
    "    def upsert_to_delta(self, micro_batch_df, batch):\n",
    "        micro_batch_df.filter(\"heartrate\" > 0).createOrReplaceTempView(self.update_temp)\n",
    "        micro_batch_df._jdf.sparkSession().sql(self.query)\n",
    "        micro_batch_df.filter(\"heartrate\" <= 0).write.format(\"delta\").mode(\"append\").saveAsTable(\"bpm_quarantine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that within the **`foreachBatch`** logic, the DataFrame operations are treating the data in each batch as if it's static rather than streaming.\n",
    "\n",
    "As such, we use the **`write`** syntax instead of **`writeStream`**.\n",
    "\n",
    "This also means that our exactly-once guarantees are relaxed. In our example above, we have two ACID transactions:\n",
    "1. Our SQL query executes to run an insert-only merge to avoid writing duplicate records to our silver table.\n",
    "2. We write a microbatch of records with negative heartrates to the **`bpm_quarantine`** table\n",
    "\n",
    "If our job fails after our first transaction completes but before the second completes, we will re-execute the full microbatch logic on job restart.\n",
    "\n",
    "However, because our insert-only merge already prevents duplicate records from being saved to our table, this will not result in any data corruption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the data and write to Gold Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data in Delta format\n",
    "df_gold=(\n",
    "spark.readStream.format(\"delta\").option(\"latestFirst\", \"true\").table(f\"{db_name}.{get_config(\"Silver\")['delta_table']}\")\n",
    "    # Apply Watermark to handle late data and perform aggeregation. See here for a discussion about withWatermark and outputMode\n",
    "    # https://dvirgiln.github.io/spark-structured-streaming-output-modes/\n",
    "    .withWatermark(\"timestamp\",\"4 minutes\")\n",
    "    .groupBy(window('eventtime',\"1 hour\"),\"Make\",\"Borough\",\"Location\",\"Month\",\"Day\",\"Hour\").count()) \\\n",
    "        .writeStream.format(\"delta\") \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .option(\"checkpointLocation\", get_config(\"Gold\")['chkpt_path']) \\\n",
    "        .start(get_config(\"Gold\")['delta_path'])\n",
    "\n",
    "# Create Delta table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {db_name}.{get_config(\"Gold\")['delta_table']}\n",
    "USING DELTA LOCATION '{get_config(\"Gold\")['delta_path']}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Live Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta Live Tables allows SQL statements to be structured as ELT pipelines. SQLs can be written as usual, but with a few differences:\n",
    "- Adding the LIVE word preceding the TABLE\n",
    "- References to DLT tables and views will always include the ```live.``` prefix. Which allows this to be automatically substituted at runtime, providing convenience for pipeline migration across DEV/QA/Prod environments\n",
    "\n",
    "Here's an example for using DLT + Auto Loader for Bronze -> Silver -> Gold incremental processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronze Zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incremental processing via <a herf=\"https://docs.databricks.com/spark/latest/structured-streaming/auto-loader.html\" target=\"_blank\">Auto Loader</a> (which uses the same processing model as Structured Streaming), requires the addition of the **`STREAMING`** keyword in the declaration as seen below. The **`cloud_files()`** method enables Auto Loader to be used natively with SQL. This method takes the following positional parameters:\n",
    "* The source location, as mentioned above\n",
    "* The source data format, which is JSON in this case\n",
    "* An arbitrarily sized array of optional reader options. In this case, we set **`cloudFiles.inferColumnTypes`** to **`true`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING LIVE TABLE sales_orders_raw\n",
    "COMMENT \"The raw sales orders, ingested from retail-org/sales_orders.\"\n",
    "AS SELECT * FROM cloud_files(\"_mnt_path_sales_orders_\", \"json\", map(\"cloudFiles.inferColumnTypes\", \"true\"));\n",
    "\n",
    "CREATE OR REFRESH STREAMING LIVE TABLE customers\n",
    "COMMENT \"The customers buying finished products, ingested from retail-org/customers.\"\n",
    "AS SELECT * FROM cloud_files(\"_mnt_path_customers_\", \"csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver Zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`CONSTRAINT`** keyword introduces quality control. Similar in function to a traditional **`WHERE`** clause, **`CONSTRAINT`** integrates with DLT, enabling it to collect metrics on constraint violations. Constraints provide an optional **`ON VIOLATION`** clause, specifying an action to take on records that violate the constraint. The three modes currently supported by DLT include:\n",
    "\n",
    "| **`ON VIOLATION`** | Behavior |\n",
    "| --- | --- |\n",
    "| **`FAIL UPDATE`** | Pipeline failure when constraint is violated |\n",
    "| **`DROP ROW`** | Discard records that violate constraints |\n",
    "| Omitted | Records violating constraints will be included (but violations will be reported in metrics) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING LIVE TABLE sales_orders_cleaned(\n",
    "  CONSTRAINT valid_order_number EXPECT (order_number IS NOT NULL) ON VIOLATION DROP ROW\n",
    ")\n",
    "COMMENT \"The cleaned sales orders with valid order_number(s).\"\n",
    "AS\n",
    "  SELECT f.customer_id, f.customer_name, f.number_of_line_items, \n",
    "         timestamp(from_unixtime((cast(f.order_datetime as long)))) as order_datetime, \n",
    "         date(from_unixtime((cast(f.order_datetime as long)))) as order_date, \n",
    "         f.order_number, f.ordered_products, c.state, c.city, c.lon, c.lat, c.units_purchased, c.loyalty_segment\n",
    "  FROM STREAM(LIVE.sales_orders_raw) f\n",
    "  LEFT JOIN LIVE.customers c\n",
    "    ON c.customer_id = f.customer_id\n",
    "    AND c.customer_name = f.customer_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the most refined level of the architecture, we declare a table delivering an aggregation with business value, in this case a collection of sales order data based in a specific region. In aggregating, the report generates counts and totals of orders by date and customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH LIVE TABLE sales_order_in_la\n",
    "COMMENT \"Sales orders in LA.\"\n",
    "AS\n",
    "  SELECT city, order_date, customer_id, customer_name, ordered_products_explode.curr, \n",
    "         sum(ordered_products_explode.price) as sales, \n",
    "         sum(ordered_products_explode.qty) as quantity, \n",
    "         count(ordered_products_explode.id) as product_count\n",
    "  FROM (SELECT city, order_date, customer_id, customer_name, explode(ordered_products) as ordered_products_explode\n",
    "        FROM LIVE.sales_orders_cleaned \n",
    "        WHERE city = 'Los Angeles')\n",
    "  GROUP BY order_date, city, customer_id, customer_name, ordered_products_explode.curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Change Data Feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change data feed allows Databricks to track row-level changes between versions of a Delta table. When enabled on a Delta table, the runtime records change events for all the data written into the table.\n",
    "\n",
    "Here are the codes for enabling this feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This enables CDF for particular tables\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE silverTable SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Or ,enable CDF using Spark conf setting in a notebook or on a cluster will ensure it's used on all newly created Delta tables in that scope.\n",
    "spark.conf.set(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this is enabled, two things will happen:\n",
    "- A folder ```_change_data``` will appear in the Delta Table directory, which contains parquet files\n",
    "- CDC data of the table can be read using the below methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Via Python\n",
    "cdc_df = (spark.readStream\n",
    "               .format(\"delta\")\n",
    "               .option(\"readChangeData\", True)\n",
    "               .option(\"startingVersion\", 0)\n",
    "               .table(\"silver\"))\n",
    "\n",
    "# Via SQL\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM table_changes('silver', 0) order by _commit_timestamp\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are using a table that has updates written to it as a streaming source! This is a **huge** value add, and something that historically has required extensive workarounds to process correctly.\n",
    "\n",
    "This would mean:\n",
    "- **Silver and Gold tables**: Updates to Silver table can be isolated and pushed to Gold table, without reading through the whole Silver table. Data removal requests can also be more easily fulfilled, as DELETE pushed down, see below cell for an example\n",
    "- **Materialized view**s: Create up-to-date, aggregated views of information for use in BI and analytics without having to reprocess the full underlying tables, instead updating only where changes have come through.\n",
    "- **Transmit changes**: Send a change data feed to downstream systems such as Kafka or RDBMS that can use it to incrementally process in later stages of data pipelines.\n",
    "- **Audit trail table**: Capture the change data feed as a Delta table provides perpetual storage and efficient query capability to see all changes over time, including when deletes occur and what updates were made.\n",
    "\n",
    "Please see [here](https://docs.databricks.com/_static/notebooks/delta/cdf-demo.html) for a demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_deletes(microBatchDF, batchId):\n",
    "    \n",
    "    (microBatchDF\n",
    "        .filter(\"_change_type = 'delete'\")\n",
    "        .createOrReplaceTempView(\"deletes\"))\n",
    "    \n",
    "    microBatchDF._jdf.sparkSession().sql(\"\"\"\n",
    "        MERGE INTO users u\n",
    "        USING deletes d\n",
    "        ON u.alt_id = d.alt_id\n",
    "        WHEN MATCHED\n",
    "            THEN DELETE\n",
    "    \"\"\")\n",
    "\n",
    "    microBatchDF._jdf.sparkSession().sql(\"\"\"\n",
    "        DELETE FROM user_bins\n",
    "        WHERE user_id IN (SELECT user_id FROM deletes)\n",
    "    \"\"\")\n",
    "    \n",
    "    microBatchDF._jdf.sparkSession().sql(\"\"\"\n",
    "        MERGE INTO delete_requests dr\n",
    "        USING deletes d\n",
    "        ON d.alt_id = dr.alt_id\n",
    "        WHEN MATCHED\n",
    "          THEN UPDATE SET status = \"deleted\"\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Commit Messages\n",
    "\n",
    "Delta Lake supports arbitrary commit messages that will be recorded to the Delta transaction log and viewable in the table history. This can help with later auditing.\n",
    "\n",
    "Setting this with SQL will create a global commit message that will be used for all subsequent operations in our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SET spark.databricks.delta.commitInfo.userMetadata=Deletes committed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With DataFrames, commit messages can also be specified as part of the write options using the **`userMetadata`** option.\n",
    "\n",
    "Here, we'll indicate that we're manually processing these requests in a notebook, rather than using an automated job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "query = (requests_df.writeStream\n",
    "                    .outputMode(\"append\")\n",
    "                    .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/delete_requests\")\n",
    "                    .option(\"userMetadata\", \"Requests processed interactively\")\n",
    "                    .trigger(availableNow=True)\n",
    "                    .table(\"delete_requests\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archiving Data\n",
    "If a company wishes to maintain an archive of historic records (but only maintain recent records in production tables), cloud-native settings for auto-archiving data can be configured to move data files automatically to lower-cost storage locations.\n",
    "\n",
    "The cell below simulates this process (here using copy instead of move). \n",
    "\n",
    "Note that because only the data files and partition directories are being relocated, the resultant table will be Parquet by default.\n",
    "\n",
    "**NOTE**: For best performance, directories should have **`OPTIMIZE`** run to condense small files to 1GB each. Because valid and stale data files are stored side-by-side in Delta Lake files, partitions should also have **`VACUUM`** executed prior to moving any Delta Lake data to a pure Parquet table to ensure only valid files are copied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_path = f\"{DA.paths.working_dir}/pii_archive\"\n",
    "source_path = f\"{DA.paths.user_db}/bronze/topic=user_info\"\n",
    "\n",
    "files = dbutils.fs.ls(source_path)\n",
    "[dbutils.fs.cp(f[0], f\"{archive_path}/{f[1]}\", True) for f in files if f[1][-8:-1] <= '2019-48'];\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS user_info_archived\n",
    "USING parquet\n",
    "LOCATION '{archive_path}'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"MSCK REPAIR TABLE user_info_archived\")\n",
    "\n",
    "display(spark.sql(\"SELECT COUNT(*) FROM user_info_archived\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll model deleting all **`user_info`** that was received before week 49 of 2019.\n",
    "\n",
    "Note that we are deleting cleanly along partition boundaries. All the data contained in the specified **`week_part`** directories will be removed from our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "DELETE FROM bronze \n",
    "WHERE topic = 'user_info'\n",
    "AND week_part <= '2019-48';\n",
    "\n",
    "VACUUM bronze RETAIN 0 HOURS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, with the deletes, the pipelines would need to be updated as well as there should now be exceptions\n",
    "> Detected deleted data from streaming source\n",
    "\n",
    "To enable streaming processing from Delta tables with partition deletes, add the **`.option(\"ignoreDeletes\", True)`** to the DataStreamReader. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "- Remove unused files from a table directory via [VACCUM](https://learn.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/language-manual/delta-vacuum/), add DRY RUN to preview previous versions to be deleted first. This is especially important for deleting PII information, as after DELETE the PII might still exist in previous images of the data\n",
    "- Compacting small files and collocate related information in the same set of files via [OPTIMIZE and ZORDER](https://www.confessionsofadataguy.com/exploring-delta-lakes-zorder-and-performance-on-databricks/).\n",
    "- Turning on Auto Optimize and Auto Compaction help us avoid the tables containing too many small files. For more information on these settings, see [here](https://docs.databricks.com/delta/optimizations/auto-optimize.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orcchestration and Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters can be added to tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the widgets API to retreive job parameters\n",
    "value = dbutils.widgets.get(\"name\")\n",
    "# Inject into context for use in the following SQL statements\n",
    "spark.conf.set(\"name\", value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "INSERT INTO task_4 VALUES ('From ${name}', current_timestamp())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce22748e8bebf5a2be273a894549765c8c30b36b7393e39d528e5cead3b97804"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
